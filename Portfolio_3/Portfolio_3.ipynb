{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Portfolio 3\n",
    "\n",
    "|Name|Github|Kaggle|\n",
    "|----|------|------|\n",
    "|Henry Lau|HenryLau08|Henry Lau|\n",
    "|Mohamed Belaachir|mobelaachir|Mo Belaachir|\n",
    "|Jayden Debi-Tewari|Jaydendt1|jaydendt123|\n",
    "|Quincy Soerohardjo|quincysoerohardjo2002|Quincy Soerohardjo|\n",
    "|Mattias Aareleid|mattyonaize|Mattias Aareleid|\n",
    "\n",
    "## Table of Contents\n",
    "- [Data Overview](#data-overview)\n",
    "- [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "- [Feature Engineering](#feature-engineering)\n",
    "- [Modeling](#modeling)\n",
    "    \n",
    "- [Results](#results)\n",
    "    - [Overview](#overview)\n",
    "    - [Scores](#scores)\n",
    "- [Conclusion & Advice](#conclusion--advice)\n",
    "- [Sources](#sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m00248.wav</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m00230.wav</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m00637.wav</td>\n",
       "      <td>hiphop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m00627.wav</td>\n",
       "      <td>metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m00138.wav</td>\n",
       "      <td>reggae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>m00192.wav</td>\n",
       "      <td>classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>m00429.wav</td>\n",
       "      <td>hiphop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>m00623.wav</td>\n",
       "      <td>reggae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m00002.wav</td>\n",
       "      <td>jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>m00039.wav</td>\n",
       "      <td>reggae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename      genre\n",
       "0  m00248.wav      metal\n",
       "1  m00230.wav    country\n",
       "2  m00637.wav     hiphop\n",
       "3  m00627.wav      metal\n",
       "4  m00138.wav     reggae\n",
       "5  m00192.wav  classical\n",
       "6  m00429.wav     hiphop\n",
       "7  m00623.wav     reggae\n",
       "8  m00002.wav       jazz\n",
       "9  m00039.wav     reggae"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sample submission voor kaggle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metal.00032.wav</td>\n",
       "      <td>classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pop.00023.wav</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classical.00076.wav</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>classical.00021.wav</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metal.00052.wav</td>\n",
       "      <td>classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>classical.00040.wav</td>\n",
       "      <td>reggae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pop.00097.wav</td>\n",
       "      <td>hiphop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>classical.00005.wav</td>\n",
       "      <td>pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>classical.00056.wav</td>\n",
       "      <td>pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>metal.00073.wav</td>\n",
       "      <td>classical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename      genre\n",
       "0      metal.00032.wav  classical\n",
       "1        pop.00023.wav      blues\n",
       "2  classical.00076.wav      blues\n",
       "3  classical.00021.wav       rock\n",
       "4      metal.00052.wav  classical\n",
       "5  classical.00040.wav     reggae\n",
       "6        pop.00097.wav     hiphop\n",
       "7  classical.00005.wav        pop\n",
       "8  classical.00056.wav        pop\n",
       "9      metal.00073.wav  classical"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_new = pd.read_csv('labels_new.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "print(\"labeled data\")\n",
    "display(labels_new.head(10))\n",
    "print()\n",
    "print(\"sample submission voor kaggle\")\n",
    "display(sample_submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is een statistische techniek die wordt gebruikt om hoog-dimensionale data te visualiseren door deze om te zetten naar een lagere dimensie, vaak 2D of 3D. Het doel is om de structuur van de data in de lagere dimensie te behouden, zodat vergelijkbare datapunten dicht bij elkaar komen te liggen en verschillende datapunten verder uit elkaar.\n",
    "\n",
    "## Werking van het t-SNE-algoritme\n",
    "\n",
    "Het t-SNE-algoritme bestaat uit twee hoofdcomponenten:\n",
    "\n",
    "1. **Constructie van de waarschijnlijkheidsverdeling in de hoog-dimensionale ruimte:**\n",
    "   - Voor elk paar van hoog-dimensionale objecten wordt een waarschijnlijkheid berekend die de gelijkenis tussen deze objecten weerspiegelt.\n",
    "   - De gelijkenis wordt gemodelleerd met behulp van een Gaussische verdeling, waarbij punten die dichter bij elkaar liggen een hogere waarschijnlijkheid krijgen en punten die verder uit elkaar liggen een lagere waarschijnlijkheid.\n",
    "\n",
    "2. **Constructie van de waarschijnlijkheidsverdeling in de laag-dimensionale ruimte en minimalisatie van de Kullback-Leibler-divergentie:**\n",
    "   - In de laag-dimensionale ruimte wordt een vergelijkbare waarschijnlijkheidsverdeling gedefinieerd, maar hier wordt de waarschijnlijkheid tussen punten berekend met behulp van een Student's t-verdeling met één vrijheidsgraad.\n",
    "   - Het algoritme minimaliseert vervolgens de Kullback-Leibler-divergentie (KL-divergentie) tussen de waarschijnlijkheidsverdelingen van de hoog-dimensionale ruimte en de laag-dimensionale ruimte door de posities van de punten in de laag-dimensionale ruimte aan te passen via gradient descent optimalisatie.\n",
    "\n",
    "## Wiskundige Formulering\n",
    "\n",
    "### Stap 1: Berekening van de waarschijnlijkheidsverdeling in de hoog-dimensionale ruimte\n",
    "\n",
    "Voor een dataset met \\( N \\) hoog-dimensionale objecten \\( \\mathbf{x}_1, \\dots, \\mathbf{x}_N \\), wordt de voorwaardelijke waarschijnlijkheid \\( p_{j|i} \\) berekend voor elk paar punten \\( \\mathbf{x}_i \\) en \\( \\mathbf{x}_j \\) (waarbij \\( i \\neq j \\)):\n",
    "\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp\\left(-\\frac{\\lVert\\mathbf{x}_i - \\mathbf{x}_j\\rVert^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{\\lVert\\mathbf{x}_i - \\mathbf{x}_k\\rVert^2}{2\\sigma_i^2}\\right)}\n",
    "$$\n",
    "\n",
    "Waarbij:\n",
    "- \\( \\mathbf{x}_i \\) en \\( \\mathbf{x}_j \\) de datapunten zijn in de hoog-dimensionale ruimte.\n",
    "- \\( \\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert^2 \\) de kwadratische Euclidische afstand is tussen de punten \\( \\mathbf{x}_i \\) en \\( \\mathbf{x}_j \\).\n",
    "- \\( \\sigma_i \\) is de bandbreedte van de Gaussische kernel voor punt \\( \\mathbf{x}_i \\).\n",
    "- De noemer normaliseert de waarschijnlijkheden zodat ze optellen tot 1 voor elk punt.\n",
    "\n",
    "De symmetrische waarschijnlijkheid \\( p_{ij} \\) wordt berekend door de voorwaardelijke waarschijnlijkheden \\( p_{j|i} \\) en \\( p_{i|j} \\) te combineren:\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}\n",
    "$$\n",
    "\n",
    "Waarbij:\n",
    "- \\( p_{ij} \\) de symmetrische waarschijnlijkheid is dat \\( \\mathbf{x}_i \\) en \\( \\mathbf{x}_j \\) dicht bij elkaar liggen.\n",
    "- \\( N \\) is het totaal aantal datapunten in de dataset.\n",
    "\n",
    "### Stap 2: Berekening van de waarschijnlijkheidsverdeling in de laag-dimensionale ruimte\n",
    "\n",
    "In de laag-dimensionale ruimte (meestal 2D of 3D) worden de waarschijnlijkheden \\( q_{ij} \\) berekend met behulp van een Student's t-verdeling met één vrijheidsgraad (wat gelijk is aan de Cauchy-verdeling). De waarschijnlijkheid tussen twee punten \\( \\mathbf{y}_i \\) en \\( \\mathbf{y}_j \\) wordt als volgt gedefinieerd:\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{(1 + \\lVert \\mathbf{y}_i - \\mathbf{y}_j \\rVert^2)^{-1}}{\\sum_{k} \\sum_{l \\neq k} (1 + \\lVert \\mathbf{y}_k - \\mathbf{y}_l \\rVert^2)^{-1}}\n",
    "$$\n",
    "\n",
    "Waarbij:\n",
    "- \\( \\mathbf{y}_i \\) en \\( \\mathbf{y}_j \\) de coördinaten van de punten zijn in de laag-dimensionale ruimte.\n",
    "- \\( \\lVert \\mathbf{y}_i - \\mathbf{y}_j \\rVert^2 \\) de kwadratische Euclidische afstand is tussen de laag-dimensionale punten \\( \\mathbf{y}_i \\) en \\( \\mathbf{y}_j \\).\n",
    "- De noemer normaliseert de waarschijnlijkheden zodat ze optellen tot 1 voor de laag-dimensionale ruimte.\n",
    "\n",
    "### Stap 3: Minimalisatie van de Kullback-Leibler-divergentie\n",
    "\n",
    "Het doel van t-SNE is om de Kullback-Leibler-divergentie tussen de twee waarschijnlijkheidsverdelingen \\( P \\) en \\( Q \\) (de verdelingen in respectievelijk de hoog-dimensionale en laag-dimensionale ruimte) te minimaliseren. Dit wordt gedaan door de volgende formule:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}(P \\parallel Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "Waarbij:\n",
    "- \\( \\mathrm{KL}(P \\parallel Q) \\) de Kullback-Leibler-divergentie is, een maat voor het verschil tussen de verdelingen \\( P \\) en \\( Q \\).\n",
    "- \\( p_{ij} \\) is de waarschijnlijkheid uit de hoog-dimensionale ruimte (berekend in stap 1).\n",
    "- \\( q_{ij} \\) is de waarschijnlijkheid uit de laag-dimensionale ruimte (berekend in stap 2).\n",
    "\n",
    "Deze KL-divergentie wordt geminimaliseerd door het optimaliseren van de posities van de punten in de laag-dimensionale ruimte via gradient descent.\n",
    "\n",
    "## Bronnen\n",
    " \n",
    " t-distributed stochastic neighbor embedding. (2024, 12 januari). Wikipedia. [Link naar Wikipedia](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n",
    " \n",
    " ML | t-Distributed Stochastic Neighbor Embedding (t-SNE) Algorithm. (z.d.). GeeksforGeeks. [Link naar GeeksforGeeks](https://www.geeksforgeeks.org/ml-t-distributed-stochastic-neighbor-embedding-t-sne-algorithm/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "## Wiskundige werking van PCA\n",
    "\n",
    "PCA is een lineaire techniek voor dimensiereductie die gebruikmaakt van een wiskundige transformatie om een dataset met hoge dimensies te projecteren op een lager-dimensionale ruimte. Het doel is om zoveel mogelijk variatie (informatie) in de data te behouden. Hieronder worden de stappen van PCA uitgelegd.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Standaardisatie van de data**\n",
    "\n",
    "De dataset wordt eerst geschaald zodat elke feature een gemiddelde van 0 en een standaardafwijking van 1 heeft. Dit gebeurt met de volgende formule:\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\bar{x}}{\\sigma}\n",
    "$$\n",
    "\n",
    "### Waarbij:\n",
    "- \\( x \\): De waarde van een specifieke feature in de dataset (een datawaarde).\n",
    "- \\( \\bar{x} \\): Het gemiddelde van alle waarden van die specifieke feature.\n",
    "- \\( \\sigma \\): De standaardafwijking van die specifieke feature.\n",
    "- \\( x' \\): De gestandaardiseerde waarde (met gemiddelde 0 en standaardafwijking 1).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Berekening van de covariantiematrix**\n",
    "\n",
    "De covariantiematrix meet de variabiliteit tussen de verschillende features. Deze wordt berekend met de formule:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n-1} X^T X\n",
    "$$\n",
    "\n",
    "### Waarbij:\n",
    "- \\( \\Sigma \\): De **covariantiematrix**, een vierkante matrix waarin elke cel de covariantie tussen twee features \\( x_i \\) en \\( x_j \\) weergeeft.\n",
    "- \\( X \\): De data **matrix** na standaardisatie. Elke rij vertegenwoordigt een datapuntenvector, en elke kolom stelt een gestandaardiseerde feature voor.\n",
    "- \\( X^T \\): De getransponeerde matrix van \\( X \\), waarin de rijen en kolommen zijn omgewisseld.\n",
    "- \\( n \\): Het aantal datapunten in de dataset.\n",
    "- \\( n-1 \\): Correctiefactor die rekening houdt met het aantal vrijheidsgraden bij het berekenen van de covariantie.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Eigenwaarden en eigenvectoren**\n",
    "\n",
    "Voor de covariantiematrix \\( \\Sigma \\) worden de eigenwaarden en eigenvectoren berekend. Deze voldoen aan de volgende eigenschap:\n",
    "\n",
    "$$\n",
    "\\Sigma v = \\lambda v\n",
    "$$\n",
    "\n",
    "### Waarbij:\n",
    "- \\( \\lambda \\): Een **eigenwaarde**, die de hoeveelheid variatie aangeeft die door de corresponderende eigenvector wordt uitgelegd.\n",
    "- \\( v \\): Een **eigenvector**, die de richting van maximale variatie in de data weergeeft.\n",
    "- \\( \\Sigma \\): De covariantiematrix.\n",
    "\n",
    "De eigenwaarden en eigenvectoren worden gebruikt om de belangrijkste componenten te bepalen.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Selectie van belangrijkste componenten**\n",
    "\n",
    "De eigenvectoren worden gerangschikt op basis van hun bijbehorende eigenwaarden \\( \\lambda \\). Alleen de top \\( k \\) eigenvectoren met de hoogste eigenwaarden worden geselecteerd. Dit bepaalt de nieuwe dimensie \\( k \\).\n",
    "\n",
    "- \\( k \\): Het aantal dimensies dat overblijft na dimensiereductie. Dit wordt gekozen door de gebruiker, afhankelijk van hoeveel variatie behouden moet blijven.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Transformatie naar de lagere dimensie**\n",
    "\n",
    "Na selectie van de belangrijkste eigenvectoren wordt de originele data getransformeerd naar de lagere dimensie. De transformatie is als volgt:\n",
    "\n",
    "$$\n",
    "Z = X W\n",
    "$$\n",
    "\n",
    "### Waarbij:\n",
    "- \\( Z \\): De getransformeerde data in de lagere-dimensionale ruimte. Elke rij is een datapuntenvector in deze nieuwe \\( k \\)-dimensionale ruimte.\n",
    "- \\( X \\): De gestandaardiseerde data matrix.\n",
    "- \\( W \\): De matrix bestaande uit de geselecteerde \\( k \\) eigenvectoren (elk als een kolom).\n",
    "\n",
    "Hierbij projecteert \\( W \\) de originele data \\( X \\) op de ruimte die wordt gedefinieerd door de \\( k \\) belangrijkste eigenvectoren.\n",
    "\n",
    "\n",
    "[(GeeksforGeeks, 2024)](https://www.geeksforgeeks.org/principal-component-analysis-pca/)\n",
    "[(Wikipedia, 2024)](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "\n",
    "# Bronnen\n",
    "\n",
    "Wikipedia contributors. (2024, 12 december). Principal Component Analysis. Wikipedia. https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "\n",
    "GeeksforGeeks. (2024, 17 september). Principal Component Analysis (PCA). GeeksforGeeks. https://www.geeksforgeeks.org/principal-component-analysis-pca/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-negatieve Matrixfactorisatie (NMF)\n",
    "\n",
    "## Wiskundige werking van NMF\n",
    "\n",
    "Non-negatieve Matrixfactorisatie (NMF) is een techniek voor dimensiereductie die een niet-negatieve matrix \\( V \\) ontleedt in twee niet-negatieve matrices \\( W \\) en \\( H \\). Het doel is om de oorspronkelijke data te benaderen door de productmatrix \\( WH \\), waarbij alle elementen niet-negatief zijn. Dit is vooral nuttig in toepassingen waar de gegevens van nature niet-negatief zijn, zoals bij afbeeldingen, audio en tekstgegevens. \n",
    "\n",
    "---\n",
    "\n",
    "### **1. Factorisatie van de matrix**\n",
    "\n",
    "NMF probeert de matrix \\( V \\) te factoriseren in twee matrices \\( W \\) en \\( H \\) zodanig dat:\n",
    "\n",
    "$$\n",
    "V \\approx WH\n",
    "$$\n",
    "\n",
    "**Betekenis van de symbolen:**\n",
    "- \\( V \\): De oorspronkelijke niet-negatieve data matrix van dimensie \\( m \\times n \\).\n",
    "- \\( W \\): Een niet-negatieve matrix van dimensie \\( m \\times k \\), waarbij elke kolom een basisvector vertegenwoordigt.\n",
    "- \\( H \\): Een niet-negatieve matrix van dimensie \\( k \\times n \\), waarbij elke rij de coëfficiënten bevat voor de lineaire combinatie van basisvectoren om de kolommen van \\( V \\) te benaderen.\n",
    "- \\( k \\): Het aantal latente componenten of basisvectoren, gekozen op basis van de gewenste reductie en interpretatie.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Optimalisatieprobleem**\n",
    "\n",
    "Het doel is om \\( W \\) en \\( H \\) te vinden die de reconstructiefout minimaliseren. Een veelgebruikte maat voor deze fout is de Frobeniusnorm van het verschil tussen \\( V \\) en \\( WH \\):\n",
    "\n",
    "$$\n",
    "\\min_{W, H} \\| V - WH \\|_F^2\n",
    "$$\n",
    "\n",
    "**Onder de voorwaarden:**\n",
    "- \\( W \\geq 0 \\)\n",
    "- \\( H \\geq 0 \\)\n",
    "\n",
    "Hierbij zorgt de niet-negativiteitsbeperking ervoor dat de resulterende matrices \\( W \\) en \\( H \\) geen negatieve elementen bevatten, wat bijdraagt aan de interpretatie van de componenten. \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Iteratieve update-algoritmen**\n",
    "\n",
    "Er zijn verschillende algoritmen ontwikkeld om \\( W \\) en \\( H \\) iteratief bij te werken om de reconstructiefout te minimaliseren. Een bekende benadering is het gebruik van multiplicatieve update-regels,  [(Lee,D,D & Seung, 2001)](https://proceedings.neurips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf\n",
    ")\n",
    "\n",
    "De update-regels zijn als volgt:\n",
    "\n",
    "$$\n",
    "H_{ij} \\leftarrow H_{ij} \\frac{(W^T V)_{ij}}{(W^T W H)_{ij}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{ij} \\leftarrow W_{ij} \\frac{(V H^T)_{ij}}{(W H H^T)_{ij}}\n",
    "$$\n",
    "\n",
    "Deze regels worden herhaald totdat de reconstructiefout convergeert naar een minimumwaarde. \n",
    "\n",
    "\n",
    "[(Wikipedia,2024)](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)\n",
    "[(GeeksforGeeks, 2024)](https://www.geeksforgeeks.org/introduction-to-non-negative-matrix-factorization/)\n",
    "\n",
    "# Bronnen\n",
    "Wikipedia contributors. (2024, 12 december). Non-negative Matrix Factorization. Wikipedia. https://en.wikipedia.org/wiki/Non-negative_matrix_factorization\n",
    "\n",
    "Lee, D. D., & Seung, H. S. (2001). Algorithms for Non-negative Matrix Factorization. Advances in Neural Information Processing Systems, 13, 556–562. https://proceedings.neurips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf\n",
    "\n",
    "\\GeeksforGeeks. (2024, 17 september). Introduction to Non-negative Matrix Factorization. GeeksforGeeks. https://www.geeksforgeeks.org/introduction-to-non-negative-matrix-factorization/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
