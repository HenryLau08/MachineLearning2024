{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Portfolio 2\n",
    "\n",
    "|Name|Github|Kaggle|\n",
    "|----|------|------|\n",
    "|Henry Lau|HenryLau08|Henry Lau|\n",
    "|Mohamed Belaachir|mobelaachir|Mo Belaachir|\n",
    "|Jayden Debi-Tewari|Jaydendt1|jaydendt123|\n",
    "|Quincy Soerohardjo|quincysoerohardjo2002|Quincy Soerohardjo|\n",
    "|Mattias Aareleid|mattyonaize|Mattias Aareleid|\n",
    "\n",
    "## Table of Contents\n",
    "- [Data Overview](#data-overview)\n",
    "- [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "- [Modeling](#modeling)\n",
    "\n",
    "- [Results](#results)\n",
    "- [Conclusion & Advice](#conclusion--advice)\n",
    "- [Sources](#sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.fft import fft, ifft\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wij hebben de kolom date_hour omgezet naar een datetime-formaat om tijdsreeksanalyses mogelijk te maken. Wanneer date_hour als een datetime-object is opgeslagen, kunnen we eenvoudig tijdsgebonden analyses uitvoeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', parse_dates=['date_hour'], index_col='date_hour')\n",
    "test = pd.read_csv('test.csv', parse_dates=['date_hour'], index_col='date_hour')\n",
    "sample_submission = pd.read_csv('sample_submission (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test data\n",
    "display(train.head(10), test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample submission for kaggle\n",
    "display(sample_submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Types:  \n",
    "\n",
    "Er zijn verschillende data types aanwezig:\n",
    "- **datetime**: date_hour\n",
    "- **Integer**: holiday, weathersit en cnt\n",
    "- **Float**: temp, atemp, hum en windspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controle op Ontbrekende Waarden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle kolommen in deze dataset hebben geen ontbrekende waarden. Dit betekent dat er geen ontbrekende data hoeft te worden aangevuld of verwijderd, wat de analyse eenvoudiger maakt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorm van de gegevens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De dataset bevat 8 kolommen (variabelen) en 16.637 rijen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kolommen beschrijven:  \n",
    "- **date_hour**: Datum en tijdstip.\n",
    "- **holiday**: Geeft aan of de datum een feestdag is (0 = geen feestdag, 1 = feestdag).\n",
    "- **weathersit**: De beschrijving van de weersomstandigheden op een schaal van 1 tot 4, waarbij 1 goed weer is en 4 heel slecht weer.\n",
    "- **temp**: Genormaliseerd temperatuur\n",
    "- **atemp**: Genormaliseerd gevoelstemperatuur\n",
    "- **hum**: Genormaliseerd luchtvochtigheid\n",
    "- **windspeed**: Genormaliseerd windsnelheid\n",
    "- **cnt**: Aantal verhuurd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Het meetniveau voor elke kolom:  \n",
    "- **date_hour**: Interval\n",
    "- **holiday**: Nominaal (binaire categorische variabele: 0 = geen feestdag, 1 = feestdag).\n",
    "- **weathersit**: Ordinaal (categorieën hebben een volgorde).\n",
    "- **temp**: Interval (continue variabele).\n",
    "- **atemp**: Interval (continue variabele).\n",
    "- **hum**: Interval (continue variabele).\n",
    "- **windspeed**: Interval (continue variabele).\n",
    "- **cnt**: Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_datetime(df):\n",
    "    data = df.copy()\n",
    "    data['year'] = data.index.year\n",
    "    data['month'] = data.index.month\n",
    "    data['week_of_year'] = data.index.isocalendar().week\n",
    "    data['day_of_year'] = data.index.day_of_year\n",
    "    data['day_of_month'] = data.index.day\n",
    "    data['day_of_week'] = data.index.weekday\n",
    "    data['hour'] = data.index.hour\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = resample_datetime(train)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualiseren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataVisualizer:\n",
    "    def __init__(self, dataframe):\n",
    "        # Ensure the dataframe has a datetime index\n",
    "        if not isinstance(dataframe.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"The dataframe index must be a DatetimeIndex.\")\n",
    "        self.df = dataframe.copy()\n",
    "        sns.set_style('darkgrid')\n",
    "\n",
    "    def plot_time_series(self, value_col, year=None, title=\"Time Series Analysis\", xlabel=\"Time\", ylabel=\"Value\"):\n",
    "        \"\"\"\n",
    "        Plots a time series for a specific column, optionally filtered by year.\n",
    "        \n",
    "        Parameters:\n",
    "            value_col (str): The column to plot.\n",
    "            year (int, optional): The year to filter the data. If None, plots all years.\n",
    "            title (str): The title of the plot.\n",
    "            xlabel (str): The label for the x-axis.\n",
    "            ylabel (str): The label for the y-axis.\n",
    "        \"\"\"\n",
    "        data_to_plot = self.df\n",
    "        if year is not None:\n",
    "            data_to_plot = data_to_plot[data_to_plot.index.year == year]\n",
    "            title += f\" ({year})\"\n",
    "        \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(data_to_plot.index, data_to_plot[value_col])\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_distributions(self, continuous_cols, bins=30):\n",
    "        n_cols = 2\n",
    "        n_rows = (len(continuous_cols) + 1) // n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 5))\n",
    "        fig.suptitle(\"Distribution of Continuous Variables\", fontsize=16)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(continuous_cols):\n",
    "            sns.histplot(self.df[col], bins=bins, kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f\"Distribution of '{col}'\")\n",
    "        \n",
    "        for j in range(len(continuous_cols), len(axes)):  # Hide extra subplots\n",
    "            fig.delaxes(axes[j])\n",
    "            \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    def plot_categorical_counts(self, categorical_cols):\n",
    "        n_cols = 2\n",
    "        n_rows = (len(categorical_cols) + 1) // n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 5))\n",
    "        fig.suptitle(\"Distribution of Categorical Variables\", fontsize=16)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(categorical_cols):\n",
    "            sns.countplot(x=self.df[col], ax=axes[i])\n",
    "            axes[i].set_title(f\"Distribution of '{col}'\")\n",
    "        \n",
    "        for j in range(len(categorical_cols), len(axes)):  # Hide extra subplots\n",
    "            fig.delaxes(axes[j])\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    def plot_boxplots(self, cols, title=\"Boxplots for Outliers\"):\n",
    "        fig, axes = plt.subplots(1, len(cols), figsize=(5 * len(cols), 6))\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "        \n",
    "        for i, col in enumerate(cols):\n",
    "            sns.boxplot(y=self.df[col], ax=axes[i])\n",
    "            axes[i].set_title(col)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    def plot_time_components_vs_cnt(self, time_components, target_col='cnt', title=\"Time Components vs. Target\"):\n",
    "        n_cols = 2\n",
    "        n_rows = (len(time_components) + 1) // n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 5))\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, component in enumerate(time_components):\n",
    "            sns.lineplot(x=component, y=target_col, data=self.df, ax=axes[i])\n",
    "            axes[i].set_title(f\"{component} vs. {target_col}\")\n",
    "        \n",
    "        for j in range(len(time_components), len(axes)):  # Hide extra subplots\n",
    "            fig.delaxes(axes[j])\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    def plot_boxplot(self, x_col, y_col, title=None, xlabel=None, ylabel=None):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.boxplot(x=self.df[x_col], y=self.df[y_col])\n",
    "        plt.title(title if title else f\"Boxplot of '{y_col}' by '{x_col}'\")\n",
    "        plt.xlabel(xlabel if xlabel else x_col)\n",
    "        plt.ylabel(ylabel if ylabel else y_col)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_correlation_matrix(self, cols, title=\"Correlation Matrix\"):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        correlation_matrix = self.df[cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the visualizer with your dataframe\n",
    "visualizer = DataVisualizer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_time_series(value_col='cnt', title=\"Tijdreeksanalyse van 'cnt'\")\n",
    "visualizer.plot_time_series(value_col='cnt', year=2011, title=\"Tijdreeksanalyse van 'cnt'\")\n",
    "visualizer.plot_time_series(value_col='cnt', year=2012, title=\"Tijdreeksanalyse van 'cnt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verdeling van de kolommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_distributions(continuous_cols=['temp', 'atemp', 'hum', 'windspeed'])\n",
    "visualizer.plot_categorical_counts(categorical_cols=['holiday', 'weathersit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relaties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_boxplot(x_col='weathersit', y_col='cnt', title=\"Boxplot van 'cnt' per weersomstandigheid\")\n",
    "visualizer.plot_boxplot(x_col='holiday', y_col='cnt', title=\"Boxplot van 'cnt' per feestdag ('holiday')\")\n",
    "visualizer.plot_correlation_matrix(cols=['temp', 'atemp', 'hum', 'windspeed', 'cnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tijdreeksanalyse van cnt:**\n",
    "\n",
    "- Het aantal cnt neemt in de loop van de tijd toe, met duidelijke schommelingen die mogelijk verband houden met seizoenen of andere tijdsgebonden factoren.\n",
    "\n",
    "**Verdelingsplots van continue variabelen:**\n",
    "\n",
    "- temp en atemp hebben een bijna normale verdeling, met een piek rond 0,6.\n",
    "- hum heeft een vrij gelijkmatige verdeling, maar met een lichte piek bij hogere luchtvochtigheidswaarden.\n",
    "- windspeed heeft een piek bij lagere waardes, wat suggereert dat de meeste cnt lage windsnelheden hebben.\n",
    "- Er blijkt heel weinig holidays te zijn\n",
    "- De verdeling bij weeromstaandigheid is er te zien dat de aantallen per waarde daalt, waarbij 1 de hoogste is. \n",
    "\n",
    "**Boxplots van cnt per weersomstandigheid:**\n",
    "\n",
    "- Het aantal cnt is het hoogst bij heldere weersomstandigheden en neemt af bij zwaardere weersomstandigheden. Dit suggereert dat slecht weer invloed kan hebben op de activiteit.\n",
    "\n",
    "**Boxplots van cnt per feestdag (holiday):**\n",
    "\n",
    "- Er zijn geen significante verschillen in cnt tussen feestdagen en gewone dagen, hoewel de variatie iets groter is op gewone dagen.\n",
    "\n",
    "**Correlatiematrix:**\n",
    "\n",
    "- temp en atemp zijn sterk gecorreleerd (0,99), wat logisch is omdat beide temperatuurgerelateerd zijn.\n",
    "- temp en atemp vertonen een matige positieve correlatie met cnt, wat aangeeft dat hogere temperaturen mogelijk gepaard gaan met meer cnt.\n",
    "- hum heeft een zwakke negatieve correlatie met cnt, wat kan betekenen dat hogere luchtvochtigheid  een negatieve invloed kan hebben op de cnt.\n",
    "- temp en atemp vertonen vaak een matige positieve correlatie met cnt. Dit wijst erop dat hogere temperaturen kunnen leiden tot een toename in het aantal cnt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controle op Uitschieters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for outliers\n",
    "visualizer.plot_boxplots(cols=[\"temp\", \"atemp\", \"hum\", \"windspeed\", \"cnt\"], \n",
    "                         title=\"Boxplots voor Uitschieter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hum: Er zijn enkele waarden dicht bij de minimumdie als outliers kunnen worden beschouwd.\n",
    "- windspeed : Er zijn een paar lage waarden die als uitschieters worden beschouwd.\n",
    "- cnt: Er zijn een paar hoge waarden die buiten het interkwartielbereik liggen en als uitschieters kunnen worden gezien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time components vs 'cnt'\n",
    "visualizer.plot_time_components_vs_cnt(time_components=['month', 'week_of_year', 'day_of_month','day_of_week', 'hour'],\n",
    "                                        target_col=\"cnt\", \n",
    "                                        title=\"Tijdscomponenten ten opzichte van 'cnt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class time_series_analysis:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def periodigram(self, col='cnt', xlim=None, ylim=None, filter_amp=None):\n",
    "        timeserie = self.df[col]\n",
    "        n = len(timeserie)\n",
    "        freq = np.fft.fftfreq(n,1)\n",
    "        fft_result = fft(timeserie)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(freq, np.abs(fft_result))\n",
    "        plt.xlabel('Frequency (1/hour)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        if xlim != None:\n",
    "            plt.xlim([0,0.1])\n",
    "        if ylim != None:\n",
    "            plt.ylim([0,1e6])\n",
    "        plt.title('Periodigram')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        df_fft = pd.DataFrame(np.abs(fft_result))\n",
    "        df_fft['freq'] = freq\n",
    "        hours = []\n",
    "        days= []\n",
    "        for f in freq:\n",
    "            if f != 0:\n",
    "                hours.append(1/f)\n",
    "                days.append(1/f/24)\n",
    "            else:\n",
    "                hours.append(np.inf)\n",
    "                days.append(np.inf)\n",
    "        df_fft['duur in uren'] = hours\n",
    "        df_fft['duur in dagen'] = days\n",
    "        df_fft.rename(columns={0:'amplitude'}, inplace=True)\n",
    "        if filter_amp != None:\n",
    "            df_fft = df_fft[(df_fft['amplitude'] > filter_amp)&(df_fft['freq'] > 0)]\n",
    "        else:\n",
    "            df_fft = df_fft[df_fft['freq'] > 0]\n",
    "        display(df_fft)\n",
    "\n",
    "    def plot_autocorrelations(self, col='', lags=24):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        # ACF plot\n",
    "        plot_acf(self.df[col], ax=axes[0], lags=lags)\n",
    "        axes[0].set_title(\"Autocorrelation Function (ACF)\")\n",
    "\n",
    "        # PACF plot\n",
    "        plot_pacf(self.df[col], ax=axes[1], lags=lags)\n",
    "        axes[1].set_title(\"Partial Autocorrelation Function (PACF)\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_seasonal_decompose(self, col='', model='additive', period=24):\n",
    "        sd = seasonal_decompose(self.df[col], model=model, period=period)\n",
    "        plt.figure(figsize=(30,6))\n",
    "        plt.title(\"Trend\")\n",
    "        sd.trend.plot()\n",
    "        plt.figure(figsize=(30,6))\n",
    "        plt.title(\"Seasonal\")\n",
    "        sd.seasonal.plot()\n",
    "        plt.figure(figsize=(30,6))\n",
    "        plt.title(\"Resid\")\n",
    "        sd.resid.plot()\n",
    "        plt.show()\n",
    "\n",
    "ts_analyzer = time_series_analysis(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seizoenspatronen met Fourier analyse\n",
    "\n",
    "##### Wat is Fourier analyse?\n",
    "Het is een wiskundig methode dat patronen vindt binnen een genormaliseerde timeseries dataset. Het vereenvoudigt complexe data door het te veranderen naar een serie van trigonomische of exponentiele functies. Door alle complicerende factors van het dataset weg te halen kunnen patronen makkelijker herkend worden, waardoor voorspellingen maken makkelijker wordt.\n",
    "\n",
    "[(Hayes, 2023)](https://www.investopedia.com/terms/f/fourieranalysis.asp) \n",
    "\n",
    "De formule van een Fourier series:\n",
    "$$\n",
    "f(t) = \\frac{a_0}{2} + \\sum_{k=1}^{\\infty}(a_k\\cos(2\\pi kt) + b_k\\sin(2\\pi kt))\n",
    "$$\n",
    "\n",
    "waar\n",
    "- $\\frac{a_0}{2}$ het constante term representeert,\n",
    "- $a_k\\cos(2\\pi kt)$ en $b_k\\sin(2\\pi kt)$ de cosinus en sinus termen zijn,\n",
    "- $k$ het harmonische frequentie is,\n",
    "- $\\sum_{k=1}^{\\infty}$ zegt dat er een oneindig serie van sinus en cosinus functies met verschillende frequenties en amplitudes opgeteld worden, die $f(t)$ preciezer zou vinden\n",
    "\n",
    "Een fourier analyse is het manier waarop functies benaderd kunnen worden door het combinatie van trigonometrische functies. \n",
    "\n",
    "[(3Blue1Brown, 2018)](https://www.youtube.com/watch?v=spUNpyF58BY )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_analyzer.periodigram(xlim=[0,0.1], ylim=[0,1e6], filter_amp=0.3e+06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_analyzer.plot_autocorrelations(col='temp', lags=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_analyzer.plot_autocorrelations(col='hum', lags=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_analyzer.plot_autocorrelations(col='windspeed', lags=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_analyzer.plot_autocorrelations(col='cnt', lags=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de visualisaties is het te zien dat de target variabele begint toe te nemen rond april, en afneemt rond de wintermaanden, beginnend met november."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_analyzer.plot_seasonal_decompose(col='cnt', model='additive', period=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onderzoek naar trends\n",
    "\n",
    "Na een Fourier analyse uit te voeren op de data, is het te zien dat de grootste trends jaarlijks en dagelijks zijn. De jaarlijkse trends zou in dit geval groei van het populariteit van het product over tijd kunnen zijn, dit ligt aan het feit dat de jaarlijkse piek in 2011 lager is dan die van 2012.\n",
    "\n",
    "De dagelijkse trend kan liggen aan andere variabelen, zoals temperatuur, weeromstandigheden of vakantie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class time_series_feature_engineering:\n",
    "    def __init__(self, train, test, target=''):\n",
    "        # Ensure the dataframe has a datetime index\n",
    "        if not isinstance(train.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"The train dataframe index must be a DatetimeIndex.\")\n",
    "        if not isinstance(test.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"The test dataframe index must be a DatetimeIndex.\")\n",
    "        \n",
    "        self.df = train.copy()\n",
    "        self.X_train = self.df.drop(target, axis=1)\n",
    "        self.y_train = self.df[target]\n",
    "        self.test = test.copy()\n",
    "\n",
    "        # Generate features for train data\n",
    "        self.X_train['year'] = self.X_train.index.year\n",
    "        self.X_train['month'] = self.X_train.index.month\n",
    "        self.X_train['week_of_year'] = self.X_train.index.isocalendar().week\n",
    "        self.X_train['week_of_year'] = self.X_train['week_of_year'].astype('int32')\n",
    "        self.X_train['day_of_year'] = self.X_train.index.day_of_year\n",
    "        self.X_train['day_of_month'] = self.X_train.index.day\n",
    "        self.X_train['day_of_week'] = self.X_train.index.weekday  # 0=Monday, 6=Sunday\n",
    "        self.X_train['hour'] = self.X_train.index.hour\n",
    "        self.X_train['is_weekend'] = self.X_train.index.weekday >= 5  # Boolean feature for weekends\n",
    "        self.X_train['quarter'] = self.X_train.index.quarter\n",
    "        self.X_train['season'] = self.X_train['month'].apply(self.get_season)\n",
    "        self.X_train['day_name'] = self.X_train.index.day_name()\n",
    "        self.X_train = pd.get_dummies(self.X_train, columns=['season','day_name'], prefix=['season','day_name'])\n",
    "        \n",
    "        # Generate features for test data\n",
    "        self.test['year'] = self.test.index.year\n",
    "        self.test['month'] = self.test.index.month\n",
    "        self.test['week_of_year'] = self.test.index.isocalendar().week\n",
    "        self.test['week_of_year'] = self.test['week_of_year'].astype('int32')\n",
    "        self.test['day_of_year'] = self.test.index.day_of_year\n",
    "        self.test['day_of_month'] = self.test.index.day\n",
    "        self.test['day_of_week'] = self.test.index.weekday\n",
    "        self.test['hour'] = self.test.index.hour\n",
    "        self.test['is_weekend'] = self.test.index.weekday >= 5\n",
    "        self.test['quarter'] = self.test.index.quarter\n",
    "        self.test['season'] = self.test['month'].apply(self.get_season)\n",
    "        self.test['day_name'] = self.test.index.day_name()\n",
    "        self.test = pd.get_dummies(self.test, columns=['season','day_name'], prefix=['season','day_name'])\n",
    "\n",
    "        # Align test and train columns\n",
    "        self.X_train, self.test = self.X_train.align(self.test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "        self.X_train = self.X_train.astype({col: 'int' for col in self.X_train.select_dtypes(include='bool').columns})\n",
    "        self.test = self.test.astype({col: 'int' for col in self.test.select_dtypes(include='bool').columns})\n",
    "\n",
    "        # Check for missing values and warn\n",
    "        self.check_missing_values(self.X_train, \"Training Data\")\n",
    "        self.check_missing_values(self.test, \"Test Data\")\n",
    "\n",
    "    def get_season(self, month):\n",
    "        \"\"\"Returns the season for a given month.\"\"\"\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Winter'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Summer'\n",
    "        elif month in [9, 10, 11]:\n",
    "            return 'Autumn'\n",
    "\n",
    "    def check_missing_values(self, df, name):\n",
    "        \"\"\"Check for missing values and print a warning if any are found.\"\"\"\n",
    "        missing_values = df.isnull().sum()\n",
    "        if missing_values.any():\n",
    "            print(f\"Warning: Missing values found in {name}:\")\n",
    "            print(missing_values[missing_values > 0])\n",
    "\n",
    "    def add_fourier_feature(self, freqs, order, constant=False, seasonal=False, drop=True):  \n",
    "        # Ensure freqs is a list\n",
    "        if not isinstance(freqs, list):\n",
    "            freqs = [freqs]  # If a single freq is provided, make it a list\n",
    "\n",
    "        fourier_terms = []\n",
    "        for freq in freqs:\n",
    "            fourier_terms.append(CalendarFourier(freq=freq, order=order))\n",
    "\n",
    "        dp = DeterministicProcess(index=self.X_train.index, constant=constant, order=order, seasonal=seasonal,\n",
    "                                        additional_terms=fourier_terms, drop=drop)\n",
    "            \n",
    "        X_train2 = dp.in_sample()\n",
    "        test2 = dp.out_of_sample(steps=len(self.test), forecast_index=self.test.index)\n",
    "\n",
    "        self.X_train = pd.concat([self.X_train, X_train2], axis=1)\n",
    "        self.test = pd.concat([self.test, test2], axis=1)\n",
    "\n",
    "        # Check for missing values after adding Fourier features\n",
    "        self.check_missing_values(self.X_train, \"Training Data\")\n",
    "        self.check_missing_values(self.test, \"Test Data\")\n",
    "    \n",
    "    def lag_feature(self, column='', lags=int):\n",
    "        \"\"\"\n",
    "        Creates lag features for a given column in the training and test datasets.\n",
    "        \n",
    "        Args:\n",
    "            column (str): The column to create lag features for.\n",
    "            lags (list or int or range): A list, range, or single integer specifying the lag values.\n",
    "        \"\"\"\n",
    "        if isinstance(lags, int):  # If a single integer is provided\n",
    "            lags = [lags]\n",
    "        elif isinstance(lags, range):  # If a range is provided\n",
    "            lags = list(lags)\n",
    "        \n",
    "        for lag in lags:\n",
    "            self.X_train[f'{column}_lag_{lag}'] = self.X_train[column].shift(lag)\n",
    "            self.test[f'{column}_lag_{lag}'] = self.test[column].shift(lag)\n",
    "\n",
    "        # Ensure rows with missing lag values are handled properly in both datasets\n",
    "        self.X_train = self.X_train.fillna(0)\n",
    "        self.test = self.test.fillna(0)\n",
    "\n",
    "        # Check for missing values after creating lag features\n",
    "        self.check_missing_values(self.X_train, \"Training Data\")\n",
    "        self.check_missing_values(self.test, \"Test Data\")\n",
    "\n",
    "ts_fe = time_series_feature_engineering(train, test, target='cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_fe.add_fourier_feature(freqs=['YE','D'], order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_fe.lag_feature('temp', 1)\n",
    "ts_fe.lag_feature('hum', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ts_fe.X_train.head(10), ts_fe.test.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ts_fe.X_train.copy()\n",
    "y_train = ts_fe.y_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors Regression\n",
    "\n",
    "De KNN algoritme is een techniek die zich richt op de waardes rond een bepaald punt. Die punt wordt dan geclassificeerd op basis van het aantal ingestelde 'nearest neighbors', oftewel naaste buren. Als je dan instelt dat je de 5 'nearest neighbors' kiest, dan wordt de waarde geclassificeerd als de meerderheid van die buren. \n",
    "\n",
    "[(IBM, 2024)](https://www.ibm.com/topics/knn)\n",
    "\n",
    "KNN Regression is een eenvoudig algoritme dat gebaseerd is op het afstand van een punt tot een bepaald aantal dichtbijzijnde punten (K). Het afstand kan op een aantal verschillende manieren gemeten worden. Ten opzichte van KNN-classificatie, in KNN-regressie is de voorspelde waarde van de datapunt gelijk aan de gemiddelde van de dichtbijzijnde punten \n",
    "(target-waardes).\n",
    "\n",
    "[(Singh, 2024)](https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/#How_Does_the_KNN_Algorithm_Work?)\n",
    "\n",
    "De afstand naar de gekozen punt wordt meestal berekent met de manhattan- of euclidean-methode. De **Euclidean** methode berekent een rechte lijn tussen de gekozen punt en een 'buur'. Euclidean afstand bereken je met de volgende formule:\n",
    "\n",
    "$$\n",
    "d(x, y) = \\sqrt{\\sum_{i=1}^{n} (y_i - x_i)^2}\n",
    "$$\n",
    "\n",
    "De **Manhattan** methode kiest voor het meten van de absolute waarde van het verschil tussen twee punten. De naam komt van de visualisatie die vaak met dit methode komt, omdat het als een rooster uitziet, net alsof je Manhattan van boven ziet. De Manhattan afstand kun je berekenen met dit methode:\n",
    "\n",
    "$$\n",
    "d(x, y) = (\\sum_{i=1}^{m} |x_i - y_i|)\n",
    "$$\n",
    "\n",
    "[(IBM, 2024)](https://www.ibm.com/topics/knn)\n",
    "\n",
    "**Standardisatie**\n",
    "\n",
    "De manier waarop KNN beslissingen neemt heeft veel te maken met het afstand tussen waardes, waardoor de schaal veel invloed heeft op de afstandsbepaling. Door de waardes te standardiseren, voorkom je dat sommige features meer voorkomen.\n",
    "\n",
    "[(Goedegebuure, 2021)](https://bookdown.org/robert_statmind/mmsc_test_01/the-k-nn-algorithm.html#standardizing-data)\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "De belangrijkste hyperparameter bij KNN is de het aantal buren (k). Een grotere k-waarde geeft een eenvoudige decision-boundary en vermindert overfitting, maar is niet handig met alle datasets. Een kleinere k-waarde kan er juist voor zorgen dat er sprake is van overfitting. \n",
    "\n",
    "[(Abdallah, 2023)](https://www.linkedin.com/pulse/improve-model-hyperparameter-tuning-k-nearest-muctary-abdallah-1e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daadwerkelijke KNN model\n",
    "\n",
    "knn_model = KNeighborsRegressor(n_neighbors=3, weights='distance', metric='manhattan')\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "knn_score = -cross_val_score(knn_model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(f\"KNN score: {knn_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beste parameters\n",
    "\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "knn_model = KNeighborsRegressor()\n",
    "knn_gridsearch = GridSearchCV(knn_model, knn_params, cv=5, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "knn_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Best Parameters and evaluation\n",
    "print(\"Best Parameters for KNN:\", knn_gridsearch.best_params_)\n",
    "\n",
    "knn_tuned = knn_gridsearch.best_estimator_\n",
    "knn_score = -cross_val_score(knn_tuned, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(\"KNN CV Score:\", knn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lineaire Regressie**\n",
    "\n",
    "Lineaire regressie is een wiskundige methode die gebruikt wordt om de relatie tussen twee variabelen te beschrijven met een rechte lijn. Deze lijn heeft de volgende vergelijking:\n",
    "[(FasterCapital. (z.d.))](https://fastercapital.com/nl/inhoud/Lineaire-regressie--hoe-u-lineaire-regressie-kunt-gebruiken-voor-doorklikmodellering-en-hoe-u-de-lineaire-relatie-kunt-vastleggen.html)\n",
    "\n",
    "$$y=a⋅x+b$$\n",
    "\n",
    "Waarbij:\n",
    "\n",
    "- $y$:  De afhankelijke variabele.\n",
    "- $x$: De onafhankelijke variabele.\n",
    "- $a$: De helling van de lijn, die laat zien hoe $y$ verandert als $x$ toeneemt.\n",
    "- $b$: Het snijpunt van de lijn met de $y-as$(intercept).\n",
    "\n",
    "Bij meer complexe datasets met meerdere onafhankelijke variabelen wordt de formule uitgebreid naar een multivariate vorm:\n",
    "\n",
    "$$\n",
    "y = a_1 \\cdot x_1 + a_2 \\cdot x_2 + \\dots + a_n \\cdot x_n + b\n",
    "$$\n",
    "\n",
    "Waarbij:\n",
    "\n",
    "- $x_1, x_2, \\dots, x_n$: De onafhankelijke variabelen.\n",
    "- $a_1, a_2, \\dots, a_n$: De hellingen.\n",
    "- $b$: Het snijpunt van de lijn met de $y-as$(intercept).\n",
    "\n",
    "Dit wordt vaak gebruikt in **meervoudige lineaire regressie** [(Tieleman (2023))](https://datasciencepartners.nl/linear-regression-assumptions/).\n",
    "\n",
    "Het doel van lineaire regressie is om een verband te laten zien tussen twee soorten gegevens: een onafhankelijke variabele en een afhankelijke variabele. Dit gebeurt door een rechte lijn te maken die zo goed mogelijk bij de gegevens past. De lijn wordt zo gekozen dat het verschil tussen de echte waarden en de voorspelde waarden zo klein mogelijk is. Op deze manier laat de lijn zien hoe de gegevens samenhangen en wat de trend is.\n",
    "\n",
    "#### **De Kleinste Kwadraten Methode(The Least Squares Method)**\n",
    "De kleinste kwadraten methode berekent de lijn die de data het best past door de residuen te minimaliseren. Een residu is het verschil tussen de werkelijke waarde ($y_i$) en de voorspelde waarde ($y=a⋅x+b$). Deze verschillen worden gekwadrateerd en opgeteld, zoals weergegeven in de formule:\n",
    "\n",
    "$$RSS=\\sum_{i=1}^{n} \\left( y_i - (a \\cdot x_i + b) \\right)^2$$\n",
    "\n",
    "\n",
    "De lijn die ervoor zorgt dat de totale afwijking zo klein mogelijk is, wordt de beste lijn genoemd.\n",
    "\n",
    "[(Wikipedia contributors, 2023)](https://en.wikipedia.org/wiki/Residual_sum_of_squares).\n",
    "\n",
    "De helling ($a$) wordt berekend met:\n",
    "\n",
    "$$\n",
    "a = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "[(Statistiek en Kansrekening 6.6: Regressie - Uitleg, z.d.)](https://info.math4all.nl/MathAdore/vd-b66-ep1.html)\n",
    "\n",
    "En het snijpunt ($b$) wordt berekend met:\n",
    "\n",
    "$$\n",
    "b = \\bar{y} - a \\cdot \\bar{x}\n",
    "$$\n",
    "\n",
    "\n",
    "Waarbij:\n",
    "\n",
    "\n",
    "- $\\bar{x}$: Het gemiddelde van alle $x_i$-waarden.\n",
    "- $\\bar{y}$: Het gemiddelde van alle $y_i$-waarden.\n",
    "- $(x_i - \\bar{x})$: Het verschil van $x_i$ ten opzichte van het gemiddelde.\n",
    "- $(y_i - \\bar{y})$: Het verschil van $y_i$​ ten opzichte van het gemiddelde.\n",
    "\n",
    "\n",
    "Deze formules helpen om de regressielijn zo te maken dat de relatie tussen $x$ en $y$ zo goed mogelijk wordt weergegeven.\n",
    "\n",
    "[(Wikipedia contributors, 2024e)](https://en.wikipedia.org/wiki/Ordinary_least_squares)\n",
    "\n",
    "#### **Loss Function**\n",
    "\n",
    "De loss-functie is een wiskundige manier om te meten hoe goed of slecht een model voorspellingen maakt. Bij lineaire regressie kijkt de loss-functie naar het verschil tussen de echte waarden ($y_i$) en de voorspelde waarden ($\\hat{y}_i$). Het doel is om dit verschil zo klein mogelijk te maken. Tijdens het trainen van het model wordt de loss-functie geminimaliseerd, zodat het model zo nauwkeurig mogelijk wordt.\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "\n",
    "De MSE wordt berekend als:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2\n",
    "$$\n",
    "\n",
    "Waarbij:\n",
    "\n",
    "- $n$: Het aantal datapunten in de dataset.\n",
    "- $y_i$: De werkelijke waarde van $y$.\n",
    "- $\\hat{y}_i$: De voorspelde waarde van $y$.\n",
    "- $\\left( y_i - \\hat{y}_i \\right)$: Het residu, oftewel het verschil tussen de werkelijke en voorspelde waarde.\n",
    "\n",
    "MSE berekent het gemiddelde van de gekwadrateerde verschillen tussen de echte waarden ($y_i$) en de voorspelde waarden ($\\hat{y}_i$). Door de fouten te kwadrateren, krijgen grotere afwijkingen meer invloed. Hierdoor is MSE gevoelig voor uitschieters.\n",
    "\n",
    "**root Mean Squared error (RMSE)**\n",
    "\n",
    "De MSE wordt berekend als:\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "RMSE is de wortel van de gemiddelde kwadratische fout (MSE) en brengt de fouten terug naar dezelfde schaal als de doelwaarden.\n",
    "\n",
    "[(Kumar (2024a))](https://vitalflux.com/mse-vs-rmse-vs-mae-vs-mape-vs-r-squared-when-to-use/)\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "De MAE wordt berekend als:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n",
    "$$\n",
    "\n",
    "\n",
    "- $\\left| y_i - \\hat{y}_i \\right|$: Absolute waarde van de fout.\n",
    "\n",
    "\n",
    "MAE berekent het gemiddelde van de absolute verschillen tussen de echte waarden en de voorspelde waarden. Omdat de fouten niet worden gekwadrateerd, is MAE minder gevoelig voor uitschieters dan MSE.\n",
    "\n",
    " [(Kumar (2024a))](https://vitalflux.com/mse-vs-rmse-vs-mae-vs-mape-vs-r-squared-when-to-use/)\n",
    " [(FasterCapital. (z.d.))](https://fastercapital.com/nl/inhoud/Lineaire-regressie--hoe-u-lineaire-regressie-kunt-gebruiken-voor-doorklikmodellering-en-hoe-u-de-lineaire-relatie-kunt-vastleggen.html)\n",
    "\n",
    "**Huber Loss**\n",
    "\n",
    "De Huber Loss is een combinatie van MSE en MAE en wordt gedefinieerd als:\n",
    "\n",
    "$$\n",
    "\\text{Huber Loss} = \n",
    "\\begin{cases} \n",
    "\\frac{1}{2} (y_i - \\hat{y}_i)^2 & \\text{als } |y_i - \\hat{y}_i| \\leq \\delta \\\\\n",
    "\\delta |y_i - \\hat{y}_i| - \\frac{1}{2} \\delta^2 & \\text{anders}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $δ$: Drempelwaarde die bepaalt wanneer MSE wordt gebruikt en wanneer MAE.\n",
    "\n",
    "Huber Loss is kwadratisch voor kleine fouten (zoals MSE) en lineair voor grote fouten (zoals MAE). Dit maakt het minder gevoelig voor uitschieters, terwijl het nog steeds de voordelen van beide methoden behoudt.\n",
    "\n",
    "[(GeeksforGeeks (2024))](https://www.geeksforgeeks.org/loss-function-for-linear-regression/)\n",
    "\n",
    "#### **Metrics**\n",
    "\n",
    "$R$ is de correlatiecoëfficiënt en laat zien hoe sterk de lineaire relatie is tussen twee variabelen ($x$ en $y$). De waarde van $R$ ligt tussen -1 en 1.\n",
    "\n",
    "$$R = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}$$\n",
    "\n",
    "waarbij:\n",
    "- $(x_i)$ en $(y_i)$ de verschillende waarden van de variabelen $x$ en  $y$ zijn.\n",
    "- $\\bar{x}$ en $\\bar{y}$ de gemiddelden van de variabelen $x$ en  $y$ zijn.\n",
    "\n",
    "Interpretatie van $R$:\n",
    "- $R=1:$ Perfect positieve correlatie.\n",
    "- $R=−1:$ Perfect negatieve correlatie.\n",
    "- $R=0:$ Geen lineaire correlatie.\n",
    "\n",
    "[(Hashmi (2022))](https://fhashmi-tech.medium.com/pearsons-r-and-coefficient-of-determination-959bd6d37bb9)\n",
    "\n",
    "$R^2$ De coëfficiënt van determinatie laat zien hoe goed de onafhankelijke variabelen de afhankelijke variabele kunnen voorspellen. De formule is:\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}$$\n",
    "\n",
    "$R2$ is het kwadraat van $R$. Dit betekent dat $R2$ altijd een positieve waarde heeft, terwijl $R$ zowel positief als negatief kan zijn.\n",
    "\n",
    "waarbij:\n",
    "- $y_i$: De werkelijke waarden.\n",
    "- $\\hat{y}_i$: De voorspelde waarden.\n",
    "- $\\bar{y}$: Het gemiddelde van de waarden.\n",
    "\n",
    "Interpretatie van $R^2$:\n",
    "\n",
    "- $R^2$=1: Het model verklaart alle variatie in $y$.\n",
    "- $R^2$=0: Het model verklaart geen enkele variatie.\n",
    "\n",
    "[(Gupta (2021))](https://medium.com/analytics-vidhya/r-squared-formula-explanation-6dc0096ce3ba)\n",
    "[(Turney (2023))](https://www.scribbr.nl/statistiek/determinatiecoefficient/)\n",
    "\n",
    "\n",
    "#### **Regularisatie**\n",
    "\n",
    "\n",
    "Regularisatie helpt om overfitting te voorkomen door een strafterm toe te voegen aan de loss function.Er zijn twee belangrijke soorten:\n",
    "\n",
    "- L1-regularisatie (Lasso):\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{lossfunctie} + \\lambda \\sum |β_i|\n",
    "$$\n",
    "\n",
    "Waarbij:\n",
    "- $λ$ een hyperparameter is die bepaalt hoeveel regularisatie er toegepast wordt. Hoe groter $λ$, hoe sterker de regularisatie.\n",
    "- $β_i$ zijn de coëfficiënten van het model \n",
    "\n",
    "Hier wordt de som van de absolute waarden van de coëfficiënten gestraft.\n",
    "\n",
    "L1-regularisatie zorgt ervoor dat sommige coëfficiënten precies nul worden, waardoor het model eenvoudiger wordt met minder variabelen. Dit is handig als je denkt dat sommige kenmerken niet belangrijk zijn en verwijderd moeten worden.\n",
    "\n",
    "- L2-regularisatie (Ridge):\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{lossfunctie} + \\lambda \\sum β_i^2\n",
    "$$\n",
    "\n",
    "Hier wordt de som van de kwadraten van de coëfficiënten gestraft.\n",
    "\n",
    "Waarbij:\n",
    "- $λ$ een hyperparameter is die bepaalt hoeveel regularisatie er toegepast wordt. Hoe groter $λ$, hoe sterker de regularisatie.\n",
    "- $β_i$ zijn de coëfficiënten van het model \n",
    "\n",
    "\n",
    "L2-regularisatie voegt een straf toe voor grote coëfficiënten, waardoor het model eenvoudiger wordt en minder gevoelig is voor veranderingen in de data. Dit helpt om overfitting te voorkomen.\n",
    "\n",
    "[(Nagpal (2024))](https://builtin.com/data-science/l2-regularization)\n",
    "\n",
    "\n",
    "- Elastic Net\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{lossfunctie} + \\lambda_1 \\sum |β_i| + \\lambda_2 \\sum β_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "Waarbij:\n",
    "\n",
    "- $β$ een parameter is die de verhouding tussen L1 en L2 bepaalt. Als $β=1$, is het puur L1 en als $β=0$, is het puur L2.\n",
    "- $λ$ is de reguliere hyperparameter.\n",
    "\n",
    "Elastic Net combineert de voordelen van zowel L1 als L2: het kan coëfficiënten naar nul brengen en tegelijkertijd de coëfficiënten reguleren. Het werkt goed wanneer er veel kenmerken zijn die met elkaar correleren.\n",
    "[(Wikipedia contributors, 2024e)](https://en.wikipedia.org/wiki/Elastic_net_regularization) [(Datacamp, 2019)](https://www.datacamp.com/tutorial/tutorial-ridge-lasso-elastic-net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation met 10 vouwen\n",
    "lr_score = -cross_val_score(\n",
    "    lr,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    ").mean()\n",
    "\n",
    "print(\"Modelresultaten:\")\n",
    "print(f\"Intercept (b0): {lr.intercept_:.2f}\")  # Geen indexering nodig\n",
    "print(f\"Coefficients (b1): {lr.coef_[0]:.2f}\")  # Enkel de eerste waarde uit de array\n",
    "print(f\"LR score: {lr_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normaliseer de data handmatig\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameters zonder 'normalize'\n",
    "lr_params = {\n",
    "    \"copy_X\": [True, False],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"n_jobs\": [1, 5, 10, 15, None],\n",
    "    \"positive\": [True, False],\n",
    "}\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_grid_search = GridSearchCV(\n",
    "    lr_model, lr_params, cv=5, scoring=\"neg_mean_squared_error\", verbose=1, n_jobs=-1\n",
    ")\n",
    "lr_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "lr_tuned = lr_grid_search.best_estimator_\n",
    "lr_score = -cross_val_score(\n",
    "    lr_tuned,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    ").mean()\n",
    "\n",
    "print(f\"Best Parameters: {lr_grid_search.best_params_}\")\n",
    "\n",
    "print(\"Modelresultaten:\")\n",
    "print(f\"Intercept (b0): {lr_tuned.intercept_:.2f}\")\n",
    "print(f\"Coefficients (b1): {lr_tuned.coef_[0]:.2f}\")\n",
    "print(f\"LR score: {lr_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(GeeksforGeeks (2024a))](https://www.geeksforgeeks.org/hyperparameter-tuning-in-linear-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor\n",
    "Een Decision Tree is een visuele beslissings- en voorspellingsmethode die werkt als een boomstructuur.  \n",
    "Het model maakt gebruik van knopen (nodes) en takken (branches) om beslissingen te nemen of voorspellingen te doen.\n",
    "\n",
    "#### Onderdelen van een Decision Tree\n",
    "- Beslissingsknopen (Decision Nodes): Stellen een voorwaarde of test voor (bijvoorbeeld Temperatuur > 5).\n",
    "- Eindknopen (End Nodes/Leaf Nodes): Geven het uiteindelijke resultaat of de voorspelling (bijvoorbeeld \"Ja\", \"Nee\" of in dit geval een getal want het is een regressie probleem).\n",
    "- Takken (Branches): Verbinden de knopen en tonen de uitkomst van de voorwaarde (bijvoorbeeld waar of onwaar).\n",
    "- De boom breidt zich uit op basis van logische voorwaarden en leidt tot verschillende mogelijke uitkomsten.  \n",
    "\n",
    "Het doel is om de resultaat van de splitsing zo \"homogeen\" mogelijk te maken. Het *criterion* specificeert de maatstaf om deze homogeniteit te beoordelen.  \n",
    "Voor regressie is de homogeniteit gebaseerd op de fout tussen de werkelijke en voorspelde waarden.  \n",
    "\n",
    "Verschillende soorten criteria voor regressie: \n",
    "- squared_error (MSE)\n",
    "- friedman_mse\n",
    "- absolute_error (MAE)\n",
    "- poisson\n",
    "\n",
    "#### Supervised Learning en Decision Trees\n",
    "Decision Trees behoren tot de categorie supervised learning:\n",
    "- Input: Historische data met kenmerken (features) en bijbehorende uitkomsten.\n",
    "- Output: Een boommodel dat nieuwe data kan gebruiken om voorspellingen te doen.  \n",
    "\n",
    "Ze kunnen omgaan met:\n",
    "- Continue variabelen: Bijvoorbeeld temperatuur- of een aantal van iets.\n",
    "- Categorische variabelen: Bijvoorbeeld classificaties als \"Regen\" of \"Geen  regen.\"  \n",
    "\n",
    "[(GeeksforGeeks, 2023a)](https://www.geeksforgeeks.org/python-decision-tree-regression-using-sklearn/)\n",
    "\n",
    "![dt](https://vitalflux.com/wp-content/uploads/2024/09/decision-tree-example.png)  \n",
    "[Decision Tree](https://vitalflux.com/wp-content/uploads/2024/09/decision-tree-example.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularisatie bij Decision Trees\n",
    "Regularisatie voegt beperkingen toe aan het leerproces van een model om overfitting te voorkomen. Bij decision trees wordt regularisatie toegepast door de complexiteit en diepte van de boom te beperken. Belangrijke technieken zijn:\n",
    "- Pre-Pruning: \n",
    "    - Maximum Depth (max_depth):\n",
    "        - Beperkt hoe diep de boom kan groeien.\n",
    "        - Een te ondiepe boom leidt tot onderfitting, terwijl een te diepe boom kan overfitten.\n",
    "\n",
    "    - Minimum Samples Split (min_samples_split):\n",
    "        - Vereist een minimum aantal samples in een knoop voordat deze wordt gesplitst.\n",
    "        - Voorkomt splitsingen die slechts op ruis in de data gebaseerd zijn.\n",
    "\n",
    "    - Minimum Samples Leaf (min_samples_leaf):\n",
    "        - Stelt een minimum aantal samples in voor een eindknoop (leaf).\n",
    "        - Zorgt ervoor dat de uiteindelijke voorspellingen niet afhangen van een zeer klein aantal data.\n",
    "\n",
    "    - Maximum Features (max_features):\n",
    "        - Beperkt het aantal kenmerken dat in overweging wordt genomen bij een splitsing.\n",
    "        - Vermindert de kans op overfitting door overmatige afhankelijkheid van specifieke kenmerken.\n",
    "\n",
    "- Post-Pruning:  \n",
    "Verwijdert delen van de boom die weinig bijdragen aan de voorspellingskracht.\n",
    "Bijvoorbeeld door middel van cost complexity pruning.\n",
    "\n",
    "[(Sopandi, 2023)](https://medium.com/@deryl.baharudin/why-we-need-to-do-regularization-in-decision-tree-machine-learning-70e77ac48b79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(criterion='squared_error', max_depth=10)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "dt_score = -cross_val_score(dt, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(\"Decision Tree CV Score:\", dt_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params = {\n",
    "    'criterion': ['squared_error', 'absolute_error', 'poisson'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "dt_model = DecisionTreeRegressor()\n",
    "dt_gridsearch = GridSearchCV(dt_model, dt_params, cv=5, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "dt_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Best Parameters and evaluation\n",
    "print(\"Best Parameters for Decision Tree:\", dt_gridsearch.best_params_)\n",
    "\n",
    "dt_tuned = dt_gridsearch.best_estimator_\n",
    "dt_score = -cross_val_score(dt_tuned, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(\"Decision Tree CV Score:\", dt_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression\n",
    "\n",
    "Een SVR model heeft als doel het voorspellen van een continu waarde, waar de waarde binnen de tolerantie-grens ϵ moet liggen van de correcte waarde. Alle voorspellingen die buiten dit grens liggen worden gepenaliseerd.\n",
    "\n",
    "De SVR model kan ook non-lineaire relaties vinden met zijn **kernel-trick functie**. Hiermee krijgt de model een extra dimensie dat complexe relaties kan onderscheiden.\n",
    "\n",
    "[(Sethi, 2024)](https://www.analyticsvidhya.com/blog/2020/03/support-vector-regression-tutorial-for-machine-learning/)\n",
    "\n",
    "### SVR model bij een lineaire relatie\n",
    "\n",
    "$$\n",
    "w*x+b = y\n",
    "$$\n",
    "\n",
    "- $w$ : vector die de helling van de lijn of hypervlak bepaalt\n",
    "- $x$ : invoerpunt\n",
    "- $b$ : bias-term, oftewel het afstand vanaf het oorsprong langs het normaalvector $w$\n",
    "- $y$ : de optimale hypervlak\n",
    "\n",
    "Een SVR wil in dit geval aan het conditie $-a < y-wx+b < a$ te voldoen, waar $a$ het beste lijn binnen de grenswaarde is. De punten binnen de grens worden gebruikt om de waarde te voorspellen.\n",
    "\n",
    "[(Gurucharan, 2020)](https://towardsdatascience.com/machine-learning-basics-support-vector-regression-660306ac5226)\n",
    "\n",
    "### Afstand en de tolerantie-grens ϵ\n",
    "\n",
    "$$\n",
    "d_{i} = |y_i - (w^tx_i + b)|\n",
    "$$\n",
    "\n",
    "- $d_{i}$ : de absolute afwijking tussen de voorspelling en de daadwerkelijke waarde\n",
    "- ϵ : de tolerantie-grens, oftewel de maximaal toegestane afwijking dat nog goed wordt gekeurd/geen penalty krijgt\n",
    "\n",
    "Voor alle punten waar $d_i$ > ϵ geldt, wordt er een penalty gegeven. De penalty wordt gegeven volgens de **loss-functie**.\n",
    "\n",
    "### Kernel trick\n",
    "\n",
    "De kernel-trick is een techniek dat gebruikt wordt bij het toepassen van de SVR algoritme, dat niet-lineair-verdeelbare data neemt en dat omzet tot in een hoger dimensie waar de data wel lineair verdeelbaar is.\n",
    "\n",
    "[(Yadav, 2023)](https://medium.com/@Suraj_Yadav/what-is-kernel-trick-in-svm-interview-questions-related-to-kernel-trick-97674401c48d)\n",
    "\n",
    "### Loss functie\n",
    "\n",
    "De loss functie van SVR wordt de lineair epsilon-insensitive loss genoemd. Hier is de loss gemeten door het afstand te bepalen tussen de gekozen waarde $y$ en de $\\epsilon$ tolerantiegrens. De wiskundige notatie van de loss function:\n",
    "\n",
    "$$\n",
    "L_{\\epsilon} =\n",
    "\\begin{cases} 0, & \\text{als } |y - f(x)| \\leq \\epsilon \\\\\n",
    "|y - f(x)| - \\epsilon, & \\text{anders} \\end{cases}\n",
    "$$\n",
    "\n",
    "- $y$ : De ware classificatie voor de datapunt\n",
    "- $f(x)$ : Het voorspelde waarde van de datapunt\n",
    "\n",
    "[(Mathworks, z.d.)](https://nl.mathworks.com/help/stats/understanding-support-vector-machine-regression.html)\n",
    "\n",
    "### Regularisatie\n",
    "\n",
    "De manier van regularisatie in de Support Vector Regression werkt voornamelijk met het regularisatie parameter C. Dit parameter controleert de afwisseling tussen het negatieve correlatie met het maximaliseren van de marge en het minimaliseren van fouten bij het voorspelling. \n",
    "\n",
    "**Hyperparameters**\n",
    "- C, het regularisatie parameter\n",
    "- epsilon/$\\epsilon$, de tolerantie-grens \n",
    "- gamma, het kernel-coefficient\n",
    "- kernel, de keuze van kernel\n",
    "- degree, het graad bij een polynomiale kern\n",
    "\n",
    "[(Van Otten, 2024)](https://spotintelligence.com/2024/05/08/support-vector-regression-svr/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daadwerkelijke model\n",
    "\n",
    "svm_model = SVR(C=100,  gamma=0.1, kernel='rbf')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_score = -cross_val_score(svm_model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(f\"SVM score: {svm_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beste parameters\n",
    "\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'epsilon': [0.001, 0.01, 0.1, 1],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1, 10, 100],\n",
    "    'degree': [2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "svm_model = SVR()\n",
    "svmgrid = GridSearchCV(svm_model, param_grid, cv=5, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "svmgrid.fit(X_train, y_train)\n",
    "print(f\"Beste parameters: {svmgrid.best_params_}\")\n",
    "\n",
    "svm_tuned = svmgrid.best_estimator_\n",
    "svm_score = -cross_val_score(svm_tuned, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(f\"SVM score: {svm_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ensembles**\n",
    "\n",
    "Ensemble learning is een techniek binnen machine learning waarbij de voorspellingen van meerdere modellen worden gecombineerd om tot een betere prestatie te komen dan een enkel model zou kunnen bereiken. Het idee is om de kracht van verschillende modellen te bundelen door hun resultaten samen te voegen. Elk model heeft zijn eigen sterke en zwakke punten, en door ze te combineren, hopen we de nauwkeurigheid, robuustheid en generalisatie van onze voorspellingen te verbeteren.\n",
    "\n",
    "Er zijn verschillende methoden om ensemble leren toe te passen:\n",
    "\n",
    "Bagging (Bootstrap Aggregating): Dit houdt in dat we meerdere versies van een model trainen op verschillende subsets van de trainingsdata. Deze subsets worden willekeurig geselecteerd met teruglegging, zodat sommige voorbeelden vaker voorkomen en andere juist helemaal niet. Elk model wordt onafhankelijk getraind, en de uiteindelijke voorspelling komt tot stand door bijvoorbeeld meerderheidsstemmen (bij classificatie) of het gemiddelde van de voorspellingen (bij regressie). Een bekend voorbeeld van een bagging-algoritme is Random Forest.\n",
    "\n",
    "Boosting: Bij boosting worden modellen na elkaar getraind. Elk nieuw model probeert de fouten van het vorige model te corrigeren door meer nadruk te leggen op de voorbeelden die eerder verkeerd geclassificeerd werden. Algoritmen zoals AdaBoost, Gradient Boosting en XGBoost maken gebruik van deze aanpak.\n",
    "\n",
    "Stacking: Stacking is een methode waarbij we meerdere basismodellen gebruiken en hun voorspellingen combineren met behulp van een meta-model. Dit meta-model wordt getraind om de output van de basismodellen zo goed mogelijk samen te voegen om tot een betere uiteindelijke voorspelling te komen.\n",
    "\n",
    "Door deze technieken toe te passen in ons project, kunnen we het risico op overfitting verminderen en de nauwkeurigheid van ons model verhogen. Ensemble-methoden zijn dan ook populair in de praktijk en worden veel gebruikt bij machine learning-competities, omdat ze vaak betere resultaten opleveren dan individuele modellen.  \n",
    "[(GeeksforGeeks, 2023h)](https://www.geeksforgeeks.org/a-comprehensive-guide-to-ensemble-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor  \n",
    "Random Forest Regression is een machine learning-techniek die gebruikt wordt voor het voorspellen van numerieke waarden. Het combineert de voorspellingen van meerdere decision trees om overfitting te verminderen en de nauwkeurigheid te verbeteren.\n",
    "\n",
    "##### Kenmerken van Random Forest\n",
    "- Het is een ensemble learning-methode: meerdere decision trees worden gecombineerd om een stabielere en meer nauwkeurige voorspelling te maken.\n",
    "- Kan gebruikt worden voor zowel classificatie- als regressieproblemen.  \n",
    "\n",
    "##### Werking\n",
    "- Bootstrap: Willekeurige subsets van rijen en features worden uit de dataset gemaakt met **teruglegging**. Elke decision tree wordt getraind op een van deze datasets.\n",
    "- Aggregatie:\n",
    "    - Classificatie: De uiteindelijke voorspelling wordt bepaald door de meerderheid van de voorspellingen (Majority voting).\n",
    "    - Regressie: Het gemiddelde van de voorspellingen van alle decision trees wordt gebruikt.  \n",
    "\n",
    "##### Voordelen\n",
    "- Vermindert variantie: Individuele decision trees hebben een hoge variantie, maar door ze te combineren, wordt het model stabieler.\n",
    "- Betrouwbaarder: Het eindresultaat is niet afhankelijk van één enkele decision tree, maar van een verzameling decision trees.  \n",
    "\n",
    "Regularisatie bij deze model is hetzelfde als bij Decision Tree Regressor, maar ook een extra parameter en dat is *n_estimators* (aantal bomen)\n",
    "\n",
    "\n",
    "De reden voor deze modelkeuze is omdat deze model geschikt is voor complexe en grote datasets, maar ook omdat deze model niet afhankelijk is van lineaire relaties.  \n",
    "Random Forest maakt gebruik van technieken zoals Bootstrap and Aggregation (bagging) om robuuste en betrouwbare voorspellingen te genereren.  \n",
    "\n",
    "[(GeeksforGeeks, 2024h)](https://www.geeksforgeeks.org/random-forest-regression-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, criterion='squared_error', max_depth=15)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf_score = -cross_val_score(rf, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(\"Random Forest CV Score:\", rf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 250, 500],\n",
    "    'criterion': ['squared_error', 'absolute_error', 'poisson'],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_gridsearch = GridSearchCV(rf_model, rf_params, cv=5, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "rf_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Best Parameters and evaluation\n",
    "print(\"Best Parameters for Random Forest:\", rf_gridsearch.best_params_)\n",
    "\n",
    "rf_tuned = rf_gridsearch.best_estimator_\n",
    "rf_score = -cross_val_score(rf_tuned, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(\"Random Forest CV Score:\", rf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gradient Boost**\n",
    "\n",
    "Gradient Boosting is een machine learning-techniek die stap voor stap beter wordt. Het begint met een simpel model dat een gemiddelde voorspelt. Daarna berekent het de fouten (de residuen) en gebruikt die om een nieuw model te trainen. Dit proces herhaalt zich totdat de fouten heel klein zijn. Het gebruikt een wiskundige techniek, genaamd gradient descent, om deze fouten steeds kleiner te maken. Hierdoor kan het sterke voorspellingen doen, maar het kan wel traag zijn bij grote datasets.  \n",
    "[(GeeksforGeeks, 2023b)](https://www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingRegressor(loss='squared_error', learning_rate=0.01, n_estimators=200, criterion='friedman_mse', max_depth=10)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "gb_score = -cross_val_score(gb_model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(f\"GB score: {gb_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_params = {\n",
    "    'loss':{'squared_error', 'absolute_error', 'huber', 'quantile'},\n",
    "    'learning_rate':{0.001, 0.01, 0.1},\n",
    "    'n_estimators':{50, 100, 250, 500},\n",
    "    'criterion':{'friedman_mse','squared_error'},\n",
    "    'min_sample_split':{2,3,4,5},\n",
    "    'min_sample_leaf':{1,2,3,4,5},\n",
    "    'max_depth':{3,5,10},\n",
    "    'max_features':{None,'sqrt','log2'},\n",
    "    'warm_start':{False, True}\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingRegressor()\n",
    "gb_gridsearch = GridSearchCV(gb_model, gb_params, cv=5, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "gb_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {gb_gridsearch.best_params_}\")\n",
    "\n",
    "gb_tuned = gb_gridsearch.best_estimator_\n",
    "\n",
    "gb_score = -cross_val_score(gb_tuned, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(f\"GB score: {gb_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **XGBoost**\n",
    "\n",
    "XGBoost is een verbeterde versie van Gradient Boosting. Het is sneller en nauwkeuriger, omdat het een techniek gebruikt die *regularisatie* heet. Dit voorkomt dat het model te ingewikkeld wordt (overfitting). XGBoost gebruikt ook slimme technieken, zoals parallel werken en efficiënt omgaan met grote hoeveelheden data. Hierdoor is het heel populair, vooral bij grote projecten.  \n",
    "[(GeeksforGeeks, 2023b)](https://www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(n_estimators=200, max_depth=15, learning_rate=0.1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_score = -cross_val_score(xgb_model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(\"XGBoost CV Score:\", xgb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective': ['reg:squarederror', 'reg:absoluteerror'],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [50, 100, 250, 500],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_child_weight': [1, 2, 3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'gamma': [0, 1, 5],\n",
    "}\n",
    "\n",
    "xgb_model = XGBRegressor(tree_method='hist') \n",
    "xgb_gridsearch = GridSearchCV(xgb_model, xgb_params, cv=5, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "xgb_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters for XGBoost:\", xgb_gridsearch.best_params_)\n",
    "\n",
    "xgb_tuned = xgb_gridsearch.best_estimator_\n",
    "xgb_score = -cross_val_score(xgb_tuned, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(\"XGBoost CV Score:\", xgb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AdaBoost**\n",
    "\n",
    "AdaBoost werkt ook stap voor stap, maar richt zich vooral op de moeilijkste fouten. Een belangrijk onderdeel van AdaBoost is de alpha-parameter. Deze parameter bepaalt hoeveel invloed elk zwak model heeft op het eindresultaat. Als een zwak model weinig fouten maakt, krijgt het een hogere alpha-waarde en heeft het meer gewicht in het totale model. Als een model veel fouten maakt, krijgt het een lagere alpha-waarde en minder invloed. Hierdoor focust AdaBoost zich op de moeilijkere delen van de data en probeert deze beter te voorspellen. Dit is handig voor simpele problemen, maar minder geschikt als er veel ruis in de data zit.  \n",
    "[(GeeksforGeeks, 2023b)](https://www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_model = AdaBoostRegressor()\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "ada_score = -cross_val_score(ada_model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(\"AdaBoost CV Score:\", ada_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_params = {\n",
    "    'n_estimators': [50, 100, 250, 500],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "    'loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "ada_model = AdaBoostRegressor()\n",
    "ada_gridsearch = GridSearchCV(ada_model, ada_params, cv=5, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "ada_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters for AdaBoost:\", ada_gridsearch.best_params_)\n",
    "\n",
    "ada_tuned = ada_gridsearch.best_estimator_\n",
    "ada_score = -cross_val_score(ada_tuned, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "print(\"AdaBoost CV Score:\", ada_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stacking**\n",
    "\n",
    "Stacking is een techniek in machine learning waarbij we meerdere modellen combineren om betere voorspellingen te maken. Het bestaat uit twee lagen:\n",
    "- Eerste laag: Hier gebruiken we verschillende \"basis-modellen\" (bijvoorbeeld een beslissingboom, een lineaire regressie, of een random forest). Deze modellen maken elk een voorspelling op basis van dezelfde data.\n",
    "- Tweede laag: De voorspellingen van de modellen uit de eerste laag worden doorgegeven aan een nieuw model, de \"meta-classifier\" of \"meta-regressor\". Dit model leert van de combinaties van voorspellingen van de eerste laag en maakt zo een nieuwe, verbeterde voorspelling.\n",
    "\n",
    "![Stacking Architecture](https://media.geeksforgeeks.org/wp-content/uploads/20200713234827/mlxtend.PNG)  \n",
    "[(GeeksforGeeks, 2021)](https://www.geeksforgeeks.org/stacking-in-machine-learning-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_models = [\n",
    "#     ('xgb', XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5)),\n",
    "#     ('gbr', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)),\n",
    "#     ('ada', AdaBoostRegressor(n_estimators=100, learning_rate=0.1)),\n",
    "# ]\n",
    "\n",
    "# # Define meta-model\n",
    "# meta_model = LinearRegression(alpha=1.0)\n",
    "\n",
    "# # Define Stacking Regressor\n",
    "# stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1)\n",
    "\n",
    "# # Fit Stacking Model\n",
    "# stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate with cross-validation\n",
    "# stacking_score = -cross_val_score(stacking_model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1).mean()\n",
    "# print(\"Stacking Model CV Score:\", stacking_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARIMA(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import pandas as pd\n",
    "\n",
    "# Autocorrelatie en gedeeltelijke autocorrelatie grafieken voor de originele tijdreeks (y_train)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Autocorrelatie grafiek (ACF)\n",
    "plt.subplot(121)\n",
    "plot_acf(y_train_split, lags=24, ax=plt.gca())\n",
    "plt.title(\"Autocorrelatie (ACF) - Trainingsdata\")\n",
    "\n",
    "# Gedeeltelijke autocorrelatie grafiek (PACF)\n",
    "plt.subplot(122)\n",
    "plot_pacf(y_train_split, lags=24, ax=plt.gca())\n",
    "plt.title(\"Gedeeltelijke Autocorrelatie (PACF) - Trainingsdata\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_analyzer.plot_autocorrelations(col='cnt', lags=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets, with a test size of 7 days *24 hours\n",
    "test_size = 7 * 24\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    ts_fe.X_train, ts_fe.y_train, test_size=test_size, random_state=42, shuffle=False)\n",
    "\n",
    "# Test of de tijdreeks stationair is met de Augmented Dickey-Fuller test\n",
    "def test_stationarity(time_series):\n",
    "    result = adfuller(time_series)\n",
    "    p_value = result[1]\n",
    "    print(f'ADF p-value: {p_value}')\n",
    "    if p_value < 0.05:\n",
    "        print(\"De tijdreeks is stationair.\")\n",
    "    else:\n",
    "        print(\"De tijdreeks is niet stationair en moet mogelijk worden gedifferentieerd.\")\n",
    "\n",
    "# Test stationariteit van de trainingsdata\n",
    "test_stationarity(y_train_split)\n",
    "\n",
    "# SARIMAX model trainen op de gedifferentieerde trainingsdata\n",
    "sarimax_model = SARIMAX(y_train_split, \n",
    "                        exog=X_train_split, \n",
    "                        order=(2, 0, 1),  \n",
    "                        seasonal_order=(2, 0, 1, 24),  \n",
    "                        enforce_stationarity=False,  \n",
    "                        enforce_invertibility=False)\n",
    "\n",
    "# Fit het model\n",
    "sarimax_result = sarimax_model.fit(disp=False)\n",
    "\n",
    "# Voorspellingen maken op de validatieset\n",
    "forecast_val = sarimax_result.predict(start=len(y_train_split), end=len(y_train_split) + len(y_val_split) - 1, exog=X_val_split)\n",
    "\n",
    "# Bereken de mean squared error (MSE)\n",
    "mse = mean_squared_error(y_val_split, forecast_val)\n",
    "\n",
    "# Bereken de root mean squared error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print de RMSE\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split de trainingsdata in een trainings- en validatieset\n",
    "train_split, val_split = train_test_split(ts_fe.df, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Formateren van data in Prophet-stijl\n",
    "train_prophet = train_split.reset_index()[['date_hour', 'cnt']].rename(columns={'date_hour': 'ds', 'cnt': 'y'})\n",
    "val_prophet = val_split.reset_index()[['date_hour', 'cnt']].rename(columns={'date_hour': 'ds', 'cnt': 'y'})\n",
    "\n",
    "#train het Prophet-model\n",
    "model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=True,\n",
    "    seasonality_prior_scale=10,  \n",
    "    changepoint_prior_scale=0.8,\n",
    "    seasonality_mode='multiplicative'  \n",
    ")\n",
    "\n",
    "model.fit(train_prophet)\n",
    "\n",
    "# Maak voorspellingen op de validatieset\n",
    "future_dates = val_prophet[['ds']]\n",
    "forecast = model.predict(future_dates)\n",
    "\n",
    "# Voorspellingen en werkelijke waarden \n",
    "y_true = val_prophet['y'].values\n",
    "y_pred = forecast['yhat'].values\n",
    "\n",
    "# RMSE berekenen\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Specificeer een grid van hyperparameters\n",
    "param_grid = {\n",
    "    'order': [(1, 1, 0), (1, 1, 1), (2, 1, 1)],\n",
    "    'seasonal_order': [(1, 1, 1, 24), (1, 1, 0, 24), (1, 0, 1, 24),(1, 1, 1, 12)],\n",
    "}\n",
    "\n",
    "# Lijst van mogelijke parametercombinaties\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Beste parameters en laagste RMSE\n",
    "best_rmse = float('inf')\n",
    "best_params = None\n",
    "\n",
    "for params in grid:\n",
    "    sarimax_model = SARIMAX(y_train_split, \n",
    "                            exog=X_train_split, \n",
    "                            order=params['order'], \n",
    "                            seasonal_order=params['seasonal_order'], \n",
    "                            enforce_stationarity=False, \n",
    "                            enforce_invertibility=False)\n",
    "    \n",
    "    sarimax_result = sarimax_model.fit(disp=False)\n",
    "    forecast_val = sarimax_result.forecast(steps=len(X_val_split), exog=X_val_split)\n",
    "    \n",
    "    # Bereken RMSE\n",
    "    rmse = mean_squared_error(y_val_split, forecast_val, squared=False)\n",
    "    \n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_params = params\n",
    "\n",
    "print(f'Beste Parameters: {best_params}')\n",
    "print(f'Beste RMSE: {best_rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_model = SARIMAX(\n",
    "    y_train_split, \n",
    "    exog=X_train_split, \n",
    "    order=(1, 0, 1),  # d=0 omdat de data stationair is\n",
    "    seasonal_order=(1, 0, 1, 25),  # D=0 om seizoensdifferentiatie uit te schakelen\n",
    "    enforce_stationarity=False,  \n",
    "    enforce_invertibility=False\n",
    ")\n",
    "\n",
    "# Fit het model\n",
    "sarimax_result = sarimax_model.fit(disp=False)\n",
    "\n",
    "# Voorspellingen maken\n",
    "forecast_val = sarimax_result.predict(\n",
    "    start=len(y_train_split), \n",
    "    end=len(y_train_split) + len(y_val_split) - 1, \n",
    "    exog=X_val_split\n",
    ")\n",
    "\n",
    "# RMSE opnieuw berekenen\n",
    "mse = mean_squared_error(y_val_split, forecast_val)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Overview\n",
    "\n",
    "#### KNN\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "#### Decision Tree\n",
    "\n",
    "#### SVR\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "#### Gradient Boost\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "#### Stacking\n",
    "\n",
    "#### SARIMA(X)\n",
    "\n",
    "#### Prophet\n",
    "\n",
    "#### Hybrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(model, test, file_name='submission'):\n",
    "    '''Makes prediction on the test data and makes a csv containing the id and predictions.\n",
    "    This is used to make a csv file to submit in the kaggle competition.\n",
    "\n",
    "    Parameters:\n",
    "        model: the trained model.\n",
    "        file_name (str): name of the csv file. On default: 'submission'\n",
    "    '''\n",
    "    TEST = test.copy()\n",
    "    y_pred = model.predict(TEST)\n",
    "    copy_test = TEST.copy()\n",
    "    copy_test['cnt'] = y_pred\n",
    "    sub = copy_test[['cnt']].reset_index()\n",
    "\n",
    "    os.makedirs('predictions', exist_ok=True)\n",
    "\n",
    "    csv_file = f'predictions/{file_name}.csv'\n",
    "    sub.to_csv(csv_file, index=False)\n",
    "    print(f'Submission saved to {csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(model, ts_fe.test, file_name=\"prophet_submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def submission(model, test, file_name='submission'):\n",
    "    '''Makes prediction on the test data and makes a csv containing the id and predictions.\n",
    "    This is used to make a csv file to submit in the kaggle competition.\n",
    "\n",
    "    Parameters:\n",
    "        model: the trained model.\n",
    "        test: the test data.\n",
    "        file_name (str): name of the csv file. On default: 'submission'\n",
    "    '''\n",
    "    TEST = test.copy()\n",
    "    \n",
    "    # Als er meerdere kolommen zijn voor exogene variabelen, selecteer dan de relevante kolom.\n",
    "    # Bijvoorbeeld, als je alleen de eerste kolom wilt gebruiken:\n",
    "    # TEST = TEST[['relevant_column_name']]  # Gebruik alleen de kolom die je nodig hebt voor exogene variabelen\n",
    "    \n",
    "    # Als je meerdere exogene variabelen hebt, gebruik dan alleen de kolom(s) die je model verwacht\n",
    "    # Hier wordt aangenomen dat de testset meerdere kolommen bevat, en we nemen alleen de eerste kolom voor exogene variabelen\n",
    "    exog_test = TEST.iloc[:, :1]  # Alleen de eerste kolom (of selecteer de juiste kolom voor je exogenen)\n",
    "    \n",
    "    # Voorspel de testdata met het getrainde model\n",
    "    y_pred = model.forecast(steps=len(TEST), exog=exog_test)  # Voorspellen op basis van de testdata\n",
    "    \n",
    "    # Voeg de voorspellingen toe aan de testdata\n",
    "    copy_test = TEST.copy()\n",
    "    copy_test['cnt'] = y_pred\n",
    "    \n",
    "    # Maak een CSV met de voorspellingen\n",
    "    sub = copy_test[['cnt']].reset_index()\n",
    "\n",
    "    # Maak de map voor de voorspellingen aan indien deze nog niet bestaat\n",
    "    os.makedirs('predictions', exist_ok=True)\n",
    "\n",
    "    # Definieer de CSV-bestandsnaam en sla de voorspellingen op\n",
    "    csv_file = f'predictions/{file_name}.csv'\n",
    "    sub.to_csv(csv_file, index=False)\n",
    "    print(f'Submission saved to {csv_file}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(sarimax_result, ts_fe.test, file_name=\"sarimax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def submission(model, test, file_name='submission'):\n",
    "    '''Makes prediction on the test data and makes a csv containing the id and predictions.\n",
    "    This is used to make a csv file to submit in the kaggle competition.\n",
    "\n",
    "    Parameters:\n",
    "        model: the trained model.\n",
    "        file_name (str): name of the csv file. On default: 'submission'\n",
    "    '''\n",
    "    TEST = test.copy()\n",
    "\n",
    "    # Format the test data to match Prophet's expected format\n",
    "    TEST_prophet = TEST.reset_index()[['date_hour']].rename(columns={'date_hour': 'ds'})\n",
    "    \n",
    "    # Make predictions using the trained model\n",
    "    y_pred = model.predict(TEST_prophet)\n",
    "\n",
    "    # Prepare the final submission format\n",
    "    copy_test = TEST.copy()\n",
    "    copy_test['cnt'] = y_pred['yhat']  # Use yhat as the predicted values\n",
    "    sub = copy_test[['cnt']].reset_index(drop=True)  # Reset index to match Kaggle format\n",
    "\n",
    "    # Create the 'predictions' directory if it doesn't exist\n",
    "    os.makedirs('predictions', exist_ok=True)\n",
    "\n",
    "    # Define the path and save the submission file\n",
    "    csv_file = f'predictions/{file_name}.csv'\n",
    "    sub.to_csv(csv_file, index=False)\n",
    "    print(f'Submission saved to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(model, ts_fe.test, file_name=\"prophet_submission\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model              |Notebook Score|Kaggle Score|\n",
    "|-------------------|--------------|------------|\n",
    "|KNN                |0             |0           |\n",
    "|Logistic Regression|0             |0           |\n",
    "|Decision Tree      |0             |0           |\n",
    "|SVR                |0             |0           |\n",
    "|Random Forest      |0             |0           |\n",
    "|Gradient Boost     |0             |0           |\n",
    "|XGBoost            |0             |0           |\n",
    "|AdaBoost           |0             |0           |\n",
    "|Stacking           |0             |0           |\n",
    "|SARIMA(X)          |0             |0           |\n",
    "|Prophet            |0             |0           |\n",
    "|hybride model      |0             |0           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tieleman, P. (2023, 6 juni). Linear Regression Assumptions: 7 assumpties bij lineaire regressie. Data Science Partners | Trainingen Data Science in Python, SQL & R. https://datasciencepartners.nl/linear-regression-assumptions/\n",
    "\n",
    "- Turney, S. (2023, 8 maart). Coefficient of Determination (R2) | Betekenis & Voorbeelden. Scribbr. https://www.scribbr.nl/statistiek/determinatiecoefficient/\n",
    "\n",
    "- GeeksforGeeks. (2024, 29 juli). Loss function for Linear regression in Machine Learning. GeeksforGeeks. https://www.geeksforgeeks.org/loss-function-for-linear-regression/\n",
    "\n",
    "- Hashmi, F. (2022, 7 januari). Pearson’s R and Coefficient of Determination - Faraz Hashmi - Medium. Medium. https://fhashmi-tech.medium.com/pearsons-r-and-coefficient-of-determination-959bd6d37bb9\n",
    "\n",
    "- FasterCapital. (z.d.). Lineaire regressie: Hoe u lineaire regressie kunt gebruiken voor doorklikmodellering en hoe u de lineaire relatie kunt vastleggen. Geraadpleegd op 24 november 2024, van https://fastercapital.com/nl/inhoud/Lineaire-regressie--hoe-u-lineaire-regressie-kunt-gebruiken-voor-doorklikmodellering-en-hoe-u-de-lineaire-relatie-kunt-vastleggen.html\n",
    "\n",
    "- Kumar, A. (2024a, augustus 18). MSE vs RMSE vs MAE vs MAPE vs R-Squared: When to Use? Analytics Yogi. https://vitalflux.com/mse-vs-rmse-vs-mae-vs-mape-vs-r-squared-when-to-use/\n",
    "\n",
    "- Nagpal, A. (2024, 3 oktober). L1 and L2 Regularization Methods, Explained. Built In. https://builtin.com/data-science/l2-regularization\n",
    "\n",
    "- Wikipedia contributors. (2024e, november 21). Elastic net regularization. Wikipedia. https://en.wikipedia.org/wiki/Elastic_net_regularization\n",
    "\n",
    "- Statistiek en kansrekening 6.6: Regressie - Uitleg. (z.d.). https://info.math4all.nl/MathAdore/vd-b66-ep1.html\n",
    "\n",
    "- Wikipedia contributors. (2024e, november 18). Ordinary least squares. Wikipedia. https://en.wikipedia.org/wiki/Ordinary_least_squares\n",
    "\n",
    "- Wikipedia contributors. (2023, 1 maart). Residual sum of squares. Wikipedia. https://en.wikipedia.org/wiki/Residual_sum_of_squares\n",
    "\n",
    "- DataCamp. (Nov 12, 2019). Ridge, Lasso, and Elastic Net Regularization: A complete tutorial. Geraadpleegd op (Nov 12, 2019), van https://www.datacamp.com/tutorial/tutorial-ridge-lasso-elastic-net\n",
    "\n",
    "- GeeksforGeeks. (2023b, mei 23). Boosting in Machine Learning | Boosting and AdaBoost. GeeksforGeeks. https://www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/\n",
    "\n",
    "- GeeksforGeeks. (2023h, december 26). A Comprehensive Guide to Ensemble Learning. GeeksforGeeks. https://www.geeksforgeeks.org/a-comprehensive-guide-to-ensemble-learning/\n",
    "\n",
    "- Gupta, S. (2021, 28 december). R-Squared: Formula Explanation - Analytics Vidhya - Medium. Medium. https://medium.com/analytics-vidhya/r-squared-formula-explanation-6dc0096ce3ba\n",
    "\n",
    "- GeeksforGeeks. (2023a, januari 11). Python | Decision Tree Regression using sklearn. GeeksforGeeks. https://www.geeksforgeeks.org/python-decision-tree-regression-using-sklearn/\n",
    "\n",
    "- GeeksforGeeks. (2024h, september 4). Random forest regression in Python. GeeksforGeeks. https://www.geeksforgeeks.org/random-forest-regression-in-python/\n",
    "\n",
    "- Sopandi, D. B. (2023, 14 september). Why We Need to Do Regularization in Decision Tree Machine Learning? Medium. https://medium.com/@deryl.baharudin/why-we-need-to-do-regularization-in-decision-tree-machine-learning-70e77ac48b79\n",
    "\n",
    "- 3Blue1Brown. (2018, 26 januari). But what is the Fourier Transform?  A visual introduction. [Video]. YouTube. https://www.youtube.com/watch?v=spUNpyF58BY\n",
    "\n",
    "- Abdallah, M. (2023, 2 mei). Improve model - Hyperparameter tuning in k-nearest neighbors. https://www.linkedin.com/pulse/improve-model-hyperparameter-tuning-k-nearest-muctary-abdallah-1e\n",
    "\n",
    "- Goedegebuure, R. (2021, 8 februari). Chapter 3 The K-NN Algorithm | Market Segmentation & Clustering. https://bookdown.org/robert_statmind/mmsc_test_01/the-k-nn-algorithm.html#standardizing-data\n",
    "\n",
    "- Gurucharan, M. K. (2021, 15 december). Machine Learning Basics: support vector regression - towards data science. Medium. https://towardsdatascience.com/machine-learning-basics-support-vector-regression-660306ac5226\n",
    "\n",
    "- Hayes, A. (2023, 4 juli). Fourier Analysis: What it Means, How it Works. Investopedia. https://www.investopedia.com/terms/f/fourieranalysis.asp\n",
    "\n",
    "- IBM. (2024, 28 oktober). KNN. IBM. https://www.ibm.com/topics/knn\n",
    "\n",
    "- Mathworks. (z.d.). Understanding support vector machine regression. https://nl.mathworks.com/help/stats/understanding-support-vector-machine-regression.html\n",
    "\n",
    "- Sethi, A. (2024, 20 november). Support Vector Regression Tutorial for Machine Learning. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2020/03/support-vector-regression-tutorial-for-machine-learning/\n",
    "\n",
    "- Singh, A. (2024, 8 oktober). KNN algorithm: Introduction to K-Nearest Neighbors Algorithm for Regression. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/#How_Does_the_KNN_Algorithm_Work?\n",
    "\n",
    "- Van Otten, N. (2024, 11 oktober). Support Vector Regression (SVR) Simplified & How To Tutorial In Python. Spot Intelligence. https://spotintelligence.com/2024/05/08/support-vector-regression-svr/\n",
    "\n",
    "- Yadav, S. (2023, 30 april). What is Kernel Trick in SVM ? Interview questions related to Kernel Trick. Medium. https://medium.com/@Suraj_Yadav/what-is-kernel-trick-in-svm-interview-questions-related-to-kernel-trick-97674401c48d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **feedback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ts_analyzer.plot_autocorrelations(col='cnt', lags=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SARIMAX**\n",
    "\n",
    "SARIMAX staat voor Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors en is een geavanceerd model dat wordt gebruikt voor het modelleren en voorspellen van tijdreeksen. Dit model breidt de  ARIMA-modellen (AutoRegressive Integrated Moving Average) uit door rekening te houden met:\n",
    "\n",
    "1.\tSeizoensgebonden patronen (Seasonal): Tijdreeksen met terugkerende patronen die zich periodiek herhalen.\n",
    "\n",
    "2.\tExogene variabelen (Exogenous regressors): Externe factoren die mogelijk invloed hebben op de tijdreeks.\n",
    "Componenten van SARIMAX\n",
    "\n",
    "#### Het SARIMAX-model bestaat uit meerdere onderldelen:\n",
    "1.\tAutoRegressieve  (AR): \n",
    "Deze term modelleert het effect van eerdere waarden van de tijdreeks (lagged values). \n",
    "\n",
    "2.\tGeïntegreerde  (I): \n",
    "Dit geeft aan hoeveel keer de tijdreeks moet worden gedifferentieerd om stationair te worden .\n",
    "\n",
    "3.\tMoving Average  (MA): \n",
    "Modelleert het effect van fouttermen (residuen) uit eerdere tijdstappen.\n",
    "\n",
    "4.\tSeizoensgebonden componenten: \n",
    "Deze bevatten dezelfde AR, I en MA-termen, maar dan specifiek voor een seizoensgebonden periode. Bijvoorbeeld, een jaarlijkse seizoenscomponent kan terugkerende patronen modelleren die jaarlijks optreden.\n",
    "\n",
    "\n",
    "5.\tExogene variabelen: \n",
    "Dit zijn extra variabelen die niet inherent deel uitmaken van de tijdreeks, maar er wel invloed op hebben. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Een SARIMAX-model wordt meestal ingevuld als:**\n",
    "SARIMAX (p, d, q)(P, D, Q, s)\n",
    "Waar:\n",
    "\n",
    "p: Het aantal lagged termen (autoregressieve termen).\n",
    "\n",
    "d: Het aantal keer dat de reeks wordt gedifferentieerd.\n",
    "\n",
    "q: Het aantal lagged fouttermen (moving average termen).\n",
    "\n",
    "P, D, Q: De respectieve AR, I, en MA-componenten voor de seizoensgebonden deelreeks.\n",
    "\n",
    "s: De lengte van de seizoensperiode.\n",
    "\n",
    "### **Hyperparameters voor SARIMAX gebaseerd op de ACF en PCF**\n",
    "\n",
    "Op basis van grafieken hebben wij de volgende hyperparameters gekozen :\n",
    "Niet-seizoensgebonden componenten (order=(p, d, q)):\n",
    "\n",
    "p=1 (vanwege significante piek in PACF bij lag 1 en daarna daling naar nul, moet AR 1 zijn ).\n",
    "\n",
    "d=0 (Uit de adf test blijk dat de data stationair, daarom d=0)\n",
    "\n",
    "q=1 (vanwege significante pieken in de ACF bij vroege lags).\n",
    "\n",
    "Seizoensgebonden componenten (seasonal_order=(P, D, Q, S)):\n",
    "\n",
    "P=1 (vanwege een piek in PACF bij lag 24).\n",
    "\n",
    "D=0 (Doordat de data stationair is)\n",
    "\n",
    "Q=1 (vanwege significante pieken in ACF bij lags zoals 24).\n",
    "\n",
    "S=24 (omdat de periodiciteit 24 uur is, gezien in de ACF verder bleek uit onze fourier analyse dat er jaarlijkse en dagelijkse trends waren hierdoor hebben wij 12 en 24 uitgebropeerd maar bleek 24 het beste ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF p-value: 7.345082905157523e-09\n",
      "De tijdreeks is stationair.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laptop\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\Laptop\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\Laptop\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 136.38192982428254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laptop\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "c:\\Users\\Laptop\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets, with a test size of 7 days *24 hours\n",
    "test_size = 7 * 24\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    ts_fe.X_train, ts_fe.y_train, test_size=test_size, random_state=42, shuffle=False)\n",
    "\n",
    "# Test of de tijdreeks stationair is met de Augmented Dickey-Fuller test\n",
    "def test_stationarity(time_series):\n",
    "    result = adfuller(time_series)\n",
    "    p_value = result[1]\n",
    "    print(f'ADF p-value: {p_value}')\n",
    "    if p_value < 0.05:\n",
    "        print(\"De tijdreeks is stationair.\")\n",
    "    else:\n",
    "        print(\"De tijdreeks is niet stationair en moet mogelijk worden gedifferentieerd.\")\n",
    "\n",
    "# Test stationariteit van de trainingsdata\n",
    "test_stationarity(y_train_split)\n",
    "\n",
    "# SARIMAX model trainen op de gedifferentieerde trainingsdata\n",
    "sarimax_model = SARIMAX(y_train_split, \n",
    "                        exog=X_train_split, \n",
    "                        order=(1, 0, 1),  \n",
    "                        seasonal_order=(1, 0, 1, 24),  \n",
    "                        enforce_stationarity=False,  \n",
    "                        enforce_invertibility=False)\n",
    "\n",
    "# Fit het model\n",
    "sarimax_result = sarimax_model.fit(disp=False)\n",
    "\n",
    "# Voorspellingen maken op de validatieset\n",
    "forecast_val = sarimax_result.predict(start=len(y_train_split), end=len(y_train_split) + len(y_val_split) - 1, exog=X_val_split)\n",
    "\n",
    "# Bereken de mean squared error (MSE)\n",
    "mse = mean_squared_error(y_val_split, forecast_val)\n",
    "\n",
    "# Bereken de root mean squared error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print de RMSE\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Facebook Prophet**\n",
    "\n",
    "Facebook Prophet is een krachtig en flexibel model voor tijdreeksanalyse dat door Facebook  is ontwikkeld. Het is speciaal ontworpen om goed om te gaan met uitdagingen die vaak voorkomen bij  tijdreeksen, zoals onregelmatigheden, seizoensgebonden patronen en ontbrekende gegevens. Prophet is vooral populair vanwege zijn gebruiksvriendelijkheid, flexibiliteit en de mogelijkheid om sterke voorspellingen te doen zonder veel parameterafstemming.\n",
    "Het model werkt volgens een additief decomposeerbaar framework, waarbij de tijdreeks wordt opgesplitst in drie onderdelem:\n",
    "1.\tTrend:\n",
    "De trend beschrijft de algemene richting van de data in de tijd. Prophet ondersteunt:\n",
    "\n",
    "Lineaire trends: Een constante groeisnelheid die geleidelijk verandert.\n",
    "\n",
    "Stukwijze lineaire trends: Hierbij kunnen er punten in de tijd (changepoints) worden geïdentificeerd waar de groeisnelheid significant verandert.\n",
    "\n",
    "Logistieke groei: Wanneer een natuurlijke limiet in de groei wordt verwacht, bijvoorbeeld vanwege verzadiging in een markt.\n",
    "\n",
    "2.\tSeizoen: Dit modelleert terugkerende patronen in de data, zoals dagelijkse, wekelijkse of jaarlijkse variaties. Seizoenscomponenten worden vaak op een flexibele manier gemodelleerd met behulp van Fourier-transformaties.\n",
    "\n",
    "3.\tFeestdagen: De feestdagencomponent is een extra laag bovenop de seizoensgebondenheid en trend. Prophet is in staat om specifieke feestdagen of speciale gebeurtenissen toe te voegen die een significante impact hebben op de data.\n",
    "\n",
    "De totale voorspelling wordt als volgt weergegeven:\n",
    "\n",
    "y(t) = g(t) + s(t) + h(t) + e(t)\n",
    "\n",
    "g(t) verwijst naar trend (veranderingen over een lange periode)\n",
    "\n",
    "s(t) verwijst naar seizoensgebondenheid (periodieke of kortetermijnveranderingen)\n",
    "\n",
    "h(t) verwijst naar de effecten van vakanties op de prognose\n",
    "\n",
    "e(t) verwijst naar de onvoorwaardelijke veranderingen die specifiek zijn voor een bedrijf of een persoon of een omstandigheid. Het wordt ook wel de error term genoemd.\n",
    "\n",
    "y(t) is de voorspelling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
